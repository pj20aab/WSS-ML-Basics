{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day-4 Decision Trees\n",
    "\n",
    "Decision Trees are very strong Machine Learning Algorith and are fundamental part of Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris['data'][:, 2:]  # petal length and width\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree_clf, out_file='iris_tree.dot', feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: dot\n"
     ]
    }
   ],
   "source": [
    "!dot -Tpng iris_tree.dot -o iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iris tree](iris_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "- **Root Node** => Top Most Node (depth = 0)\n",
    "- **Child Nodes** => All Nodes below Root Node with Childs\n",
    "- **Leaf Node**  => Node that does not have any Child Nodes\n",
    "\n",
    "- **Samples** => how many training instances does the split apply to\n",
    "- **Value** => Tell about how many training instances of each class\n",
    "- **Gini** => Measures the Impurity\n",
    "\n",
    "Gini = 0 ('Pure') all training instances it applies belongs to the same class\n",
    "\n",
    "**Gi = 1 - Summation_k(pik^2)**\n",
    "\n",
    "where pik is the ratio of class k instances amoung the training instances of the ith node\n",
    "\n",
    "Example for depth 2 and class veriscolor - Gini = 1 - (0/54)^2 - (49/54)^2 - (5/54)^2 ~ 0.168"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**- Sklearn uses CART algorith which only produces Binary Trees (yes/no). However other algorithms like ID3 can produce Decision Trees which have nodes with more than two child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees can also return probabilities. To do so they first traverse through Tree to reach the leaf node and then return the ratio of trainig instaces of the class k (like 49/54 for Versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Training Algorithm\n",
    "\n",
    "**Classification and Regression Tree Algorithm** works by splitting training set into subsets using single feature k ans a threshold tk (like petal length <= 2.45). It searches for k and tk that produce purest subsets. It tries to **mininmise**\n",
    "\n",
    "**J(k, tk) = (m_left/m)\\*G_left + (m_right/m)\\*G_right**  (Cost Function)\n",
    "\n",
    "where G is the ginni value (impurity) and m is the number of training examples in left/right subset\n",
    "\n",
    "max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_left and max_leaf_nodes can be used to control the depth of the tree.\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "Finding the **Optimal Solution** requires O(exp(m)) time which makes it very difficult even for small datasets thus we have to settle with **Reasonable Good** solution.\n",
    "\n",
    "Making Prediction requires going through O(log2(m)) nodes. Since each node requires checking value of one feature the over complexity is O(log2(m)) which is indipendent of number of features.\n",
    "\n",
    "Training Complexity is about O(n x m log2(m))\n",
    "\n",
    "#### Gini vs Entropy\n",
    "\n",
    "Entropy can also be used instead of Gini\n",
    "\n",
    "**Entropy = - Summation_k(pik\\*log2(pik))** where, pik != 0\n",
    "\n",
    "Usually both produce same results, Gini is a bit fast to compute. Gini isolates most frequent classes into there own brach of trees whereas entropy tends to produce overall more balanced trees.\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "**max_depth**, min_samples_split (min number of samples a node must have before splitting), min_samples_leaf (min number of samples a leaf node must have before splitting), min_weight_fraction_leaf (same as min_samples_leaf just the ratio is used), max_leaf_nodes (max number of leaf nodes) and max_features can be used to regularize the decision tree. Incresing the min_ hyperparameter or decreasing max_ hyperparameter.\n",
    "\n",
    "Many algorithms first train decision trees without restrictions, then pruning (deleting) unnecessary nodes. This can be done by using chi square tests etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree_reg, out_file='iris_tree_reg.dot', feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: dot\n"
     ]
    }
   ],
   "source": [
    "!dot -Tpng iris_tree_reg.dot -o iris_tree_reg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree reg](iris_tree_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Instead of predicting a class it predicts a value, which is the average target value of all training examples in that node.\n",
    "- #### Instead of Gini Function it uses MSE\n",
    "\n",
    "\n",
    "**J(k, tk) = (m_left/m)\\*MSE_left + (m_right/m)\\*MSE_right**  (Cost Function)\n",
    "\n",
    "where, **MSE_node = Summation_i(y_node - y_i)^2** and **y_node = (1/m_node)\\*Summation_i(y_i)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of Decision Trees\n",
    "\n",
    "- They have orthogonal boundrie which makes them sensitive to training set rotation. To help with this techniques like PCA can be used.\n",
    "- Very Sensitive to small variations in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Dichotomiser 3 (ID3)\n",
    "\n",
    "The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C4.5\n",
    "\n",
    "C4.5 is similar to ID3, but removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. This accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C5\n",
    "\n",
    "C5.0 is under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi_Square Automatic Interaction Detection (CHAID)\n",
    "\n",
    "To find the most dominant feature, chi-square tests will use that is also called CHAID whereas ID3 uses information gain, C4.5 uses gain ratio and CART uses the GINI index.\n",
    "\n",
    "The formula of chi-square:-\n",
    "\n",
    "√((y – y’)2 / y’)\n",
    "\n",
    "where y is actual and y’ is expected.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
