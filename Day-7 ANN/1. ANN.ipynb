{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Neurons of previous layer are fully connected to all neurons of the next layer => **Dense Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Perceptron Example\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # Petal Length, Width\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron Model (MLP)/ Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Functions**\n",
    "* Step Function (Linear hence Gradient Descent cannot work)\n",
    "* Sigmoid (sigma(z) = 1/(1+exp(-z))\n",
    "* Hyperbolic Tangent (tanh(z) = 2sigma(2z) -1)\n",
    "* ReLU (Rectified Linear Unit Function) (ReLU(z) = max(0, z))\n",
    "* Soft Plus (softplus(z) = log(1_exp(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Functions**\n",
    "\n",
    "*Regression*\n",
    "* Mean Squared Error\n",
    "* Mean Absolute Error (If dataset has many outliers)\n",
    "* Huber Loss (Combination of Mean Squared and Absolute)\n",
    "\n",
    "*Classification*\n",
    "* Cross-Entropy Loss (Log Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classifier using Sequential API\n",
    "\n",
    "**Fashion MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 15s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 3s 1us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting the Dataset to Training & Validation mapping pixel values to 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/250.0, X_train_full[5000:]/250.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))  # reshapes data to 1D layer\n",
    "model.add(keras.layers.Dense(300, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))  # 10 Output Classes, Softmax because classes are exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**or**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = keras.models.Sequential([\n",
    "\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    \n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    \n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    \n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()  # Get Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x7fbaf211b460>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fbaf2109b80>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fbaf2110c10>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fbaf20fb760>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers  # Get Layers as Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access Parameters of Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, baises = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00512695,  0.01921242,  0.03789552, ...,  0.02957894,\n",
       "         0.01183084,  0.06524068],\n",
       "       [ 0.00702969,  0.04574708,  0.04260324, ...,  0.06414644,\n",
       "         0.07084455, -0.06259472],\n",
       "       [ 0.067012  , -0.02496573, -0.01180158, ...,  0.07188828,\n",
       "        -0.07353683,  0.05737053],\n",
       "       ...,\n",
       "       [ 0.01838095,  0.05082804,  0.06326634, ...,  0.05495444,\n",
       "        -0.05183962, -0.04725914],\n",
       "       [ 0.03009085, -0.07114021, -0.03000849, ...,  0.05728084,\n",
       "         0.01274969, -0.0670065 ],\n",
       "       [ 0.0387482 , -0.07418387, -0.05091747, ...,  0.05750048,\n",
       "         0.01864098,  0.04134686]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baises.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "             optimizer='sgd',  # Stochastic Gradient Descent (default learning rate = 0.01)\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choices for Loss Functions**\n",
    "* Sparse Categorical Crossentropy: For Sparse Labels and exclusive classes\n",
    "* Categorical Crossentrpy: One Target Probability per Class (One-Hot Vectors)\n",
    "* Binary Crossentropy: Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.9980 - accuracy: 0.6870 - val_loss: 0.5208 - val_accuracy: 0.8144\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5049 - accuracy: 0.8246 - val_loss: 0.4806 - val_accuracy: 0.8336\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4496 - accuracy: 0.8445 - val_loss: 0.4353 - val_accuracy: 0.8510\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4180 - accuracy: 0.8527 - val_loss: 0.4127 - val_accuracy: 0.8562\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3968 - accuracy: 0.8599 - val_loss: 0.3932 - val_accuracy: 0.8624\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3846 - accuracy: 0.8640 - val_loss: 0.3943 - val_accuracy: 0.8602\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3691 - accuracy: 0.8690 - val_loss: 0.3641 - val_accuracy: 0.8746\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3575 - accuracy: 0.8742 - val_loss: 0.4054 - val_accuracy: 0.8562\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3430 - accuracy: 0.8771 - val_loss: 0.3534 - val_accuracy: 0.8768\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3350 - accuracy: 0.8795 - val_loss: 0.3503 - val_accuracy: 0.8768\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3256 - accuracy: 0.8843 - val_loss: 0.3406 - val_accuracy: 0.8822\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3121 - accuracy: 0.8911 - val_loss: 0.3290 - val_accuracy: 0.8800\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3138 - accuracy: 0.8880 - val_loss: 0.3292 - val_accuracy: 0.8838\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3035 - accuracy: 0.8919 - val_loss: 0.3169 - val_accuracy: 0.8898\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2987 - accuracy: 0.8939 - val_loss: 0.3277 - val_accuracy: 0.8810\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2917 - accuracy: 0.8946 - val_loss: 0.3252 - val_accuracy: 0.8850\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2881 - accuracy: 0.8948 - val_loss: 0.3137 - val_accuracy: 0.8850\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2744 - accuracy: 0.9008 - val_loss: 0.3238 - val_accuracy: 0.8818\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2759 - accuracy: 0.8992 - val_loss: 0.3097 - val_accuracy: 0.8904\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2736 - accuracy: 0.9018 - val_loss: 0.3077 - val_accuracy: 0.8882\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2652 - accuracy: 0.9027 - val_loss: 0.3218 - val_accuracy: 0.8794\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2580 - accuracy: 0.9058 - val_loss: 0.3237 - val_accuracy: 0.8834\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2522 - accuracy: 0.9093 - val_loss: 0.3025 - val_accuracy: 0.8924\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2537 - accuracy: 0.9088 - val_loss: 0.2972 - val_accuracy: 0.8928\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2450 - accuracy: 0.9114 - val_loss: 0.2995 - val_accuracy: 0.8938\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2422 - accuracy: 0.9133 - val_loss: 0.3073 - val_accuracy: 0.8876\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2407 - accuracy: 0.9125 - val_loss: 0.3256 - val_accuracy: 0.8792\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2313 - accuracy: 0.9161 - val_loss: 0.2944 - val_accuracy: 0.8938\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2302 - accuracy: 0.9163 - val_loss: 0.3116 - val_accuracy: 0.8870\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2237 - accuracy: 0.9188 - val_loss: 0.2983 - val_accuracy: 0.8928\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* *Validation ratio can also be given by `validation_split=0.1` instead of manually spliting and giving it.*\n",
    "* *Custom `class_weight` can also be given if the dataset is skewed.*\n",
    "* *Instance weights can also be given if some part of dataset is expected to be more accurate than other.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)  # Set vertical limit from 0 to 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 77.1906 - accuracy: 0.8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[77.19056701660156, 0.8274999856948853]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regression MLP using Sequential API\n",
    "\n",
    "**California Housing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0566 - val_loss: 0.6918\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.6374 - val_loss: 0.5940\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.6564 - val_loss: 0.4904\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5147 - val_loss: 0.4650\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1000us/step - loss: 0.4995 - val_loss: 0.4222\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.4170 - val_loss: 0.4072\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.3961 - val_loss: 0.4070\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 773us/step - loss: 0.3914 - val_loss: 0.3953\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.4035 - val_loss: 0.3931\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.3907 - val_loss: 0.3866\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 831us/step - loss: 0.3815 - val_loss: 0.3864\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 778us/step - loss: 0.4032 - val_loss: 0.3912\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.4002 - val_loss: 0.3774\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.3750 - val_loss: 0.3794\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 797us/step - loss: 0.3645 - val_loss: 0.3721\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - val_loss: 0.3758\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.3686\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.3679 - val_loss: 0.3684\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 963us/step - loss: 0.3715 - val_loss: 0.3640\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3651 - val_loss: 0.3617\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)  # Set vertical limit from 0 to 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3569\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Model using Functional API\n",
    "\n",
    "Input Layer is connected to Dense Layer inside the model as well as to the Output Layer. This allows the model to both learn deep patterns (like sequential models) and simple rules.\n",
    "\n",
    "*Functional API because it is called like a function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Layer Conneceted to Dense Model (Deep Path) as well as Output (Wide Path)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])  # Concatenates the results from the deep layers are well as directly from input layer\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inputs are devided, few features are sent throuth Deep Path others are sent through Wide Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA = keras.layers.Input(shape=[5], name='wide_input')  # Takes 5 Features\n",
    "inputB = keras.layers.Input(shape=[6], name='deep_input')  # Takes 6 Features\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(inputB)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([inputA, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "\n",
    "model = keras.Model(inputs=[inputA, inputB], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input Data is added as a Tuple*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 3.1129 - val_loss: 0.8878\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 908us/step - loss: 0.8376 - val_loss: 0.7117\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 896us/step - loss: 0.7088 - val_loss: 0.6580\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 874us/step - loss: 0.6632 - val_loss: 0.6274\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 888us/step - loss: 0.6107 - val_loss: 0.6044\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 897us/step - loss: 0.5845 - val_loss: 0.5848\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.5720 - val_loss: 0.5673\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 852us/step - loss: 0.5656 - val_loss: 0.5532\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 893us/step - loss: 0.5505 - val_loss: 0.5414\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5430 - val_loss: 0.5315\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.5213 - val_loss: 0.5231\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 934us/step - loss: 0.4913 - val_loss: 0.5158\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.4823 - val_loss: 0.5085\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 927us/step - loss: 0.4987 - val_loss: 0.5026\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 931us/step - loss: 0.4855 - val_loss: 0.4954\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 981us/step - loss: 0.4757 - val_loss: 0.4904\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 880us/step - loss: 0.4681 - val_loss: 0.4850\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.4584 - val_loss: 0.4806\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 977us/step - loss: 0.4680 - val_loss: 0.4742\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 888us/step - loss: 0.4706 - val_loss: 0.4704\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 869us/step - loss: 0.4432\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0852504],\n",
       "       [2.0251963],\n",
       "       [3.3877177]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple Outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA = keras.layers.Input(shape=[5], name='wide_input')  # Takes 5 Features\n",
    "inputB = keras.layers.Input(shape=[6], name='deep_input')  # Takes 6 Features\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(inputB)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([inputA, hidden2])\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "\n",
    "model = keras.Model(inputs=[inputA, inputB], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Each Output Layer can have individual Loss Function.*\n",
    "* *By Default, final loss would be the sum of losses ot all output layers. But, in some cases, one output might be of more importance, hence weights of outputs can be given.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Output Labels are added in tuples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.5309 - main_output_loss: 1.3654 - aux_output_loss: 3.0208 - val_loss: 0.8613 - val_main_output_loss: 0.8081 - val_aux_output_loss: 1.3401\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6542 - main_output_loss: 0.5901 - aux_output_loss: 1.2315 - val_loss: 0.5741 - val_main_output_loss: 0.5244 - val_aux_output_loss: 1.0214\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5351 - main_output_loss: 0.4854 - aux_output_loss: 0.9825 - val_loss: 0.5072 - val_main_output_loss: 0.4692 - val_aux_output_loss: 0.8498\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4845 - main_output_loss: 0.4464 - aux_output_loss: 0.8279 - val_loss: 0.5001 - val_main_output_loss: 0.4723 - val_aux_output_loss: 0.7500\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4508 - main_output_loss: 0.4207 - aux_output_loss: 0.7220 - val_loss: 0.4534 - val_main_output_loss: 0.4284 - val_aux_output_loss: 0.6782\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4348 - main_output_loss: 0.4080 - aux_output_loss: 0.6766 - val_loss: 0.4430 - val_main_output_loss: 0.4219 - val_aux_output_loss: 0.6328\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4622 - main_output_loss: 0.4404 - aux_output_loss: 0.6580 - val_loss: 0.4320 - val_main_output_loss: 0.4137 - val_aux_output_loss: 0.5967\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4272 - main_output_loss: 0.4075 - aux_output_loss: 0.6049 - val_loss: 0.4399 - val_main_output_loss: 0.4236 - val_aux_output_loss: 0.5871\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4273 - main_output_loss: 0.4080 - aux_output_loss: 0.6008 - val_loss: 0.4247 - val_main_output_loss: 0.4089 - val_aux_output_loss: 0.5670\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4226 - main_output_loss: 0.4039 - aux_output_loss: 0.5905 - val_loss: 0.4034 - val_main_output_loss: 0.3879 - val_aux_output_loss: 0.5426\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4092 - main_output_loss: 0.3911 - aux_output_loss: 0.5727 - val_loss: 0.3998 - val_main_output_loss: 0.3852 - val_aux_output_loss: 0.5311\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3970 - main_output_loss: 0.3796 - aux_output_loss: 0.5536 - val_loss: 0.3969 - val_main_output_loss: 0.3829 - val_aux_output_loss: 0.5230\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - main_output_loss: 0.3651 - aux_output_loss: 0.5277 - val_loss: 0.3867 - val_main_output_loss: 0.3727 - val_aux_output_loss: 0.5123\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - main_output_loss: 0.3663 - aux_output_loss: 0.5182 - val_loss: 0.3837 - val_main_output_loss: 0.3700 - val_aux_output_loss: 0.5072\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3966 - main_output_loss: 0.3820 - aux_output_loss: 0.5280 - val_loss: 0.3819 - val_main_output_loss: 0.3691 - val_aux_output_loss: 0.4977\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3727 - main_output_loss: 0.3577 - aux_output_loss: 0.5078 - val_loss: 0.3720 - val_main_output_loss: 0.3596 - val_aux_output_loss: 0.4837\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3814 - main_output_loss: 0.3682 - aux_output_loss: 0.5002 - val_loss: 0.3703 - val_main_output_loss: 0.3586 - val_aux_output_loss: 0.4761\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3665 - main_output_loss: 0.3529 - aux_output_loss: 0.4888 - val_loss: 0.3658 - val_main_output_loss: 0.3543 - val_aux_output_loss: 0.4699\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3543 - main_output_loss: 0.3417 - aux_output_loss: 0.4677 - val_loss: 0.3655 - val_main_output_loss: 0.3543 - val_aux_output_loss: 0.4662\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - main_output_loss: 0.3402 - aux_output_loss: 0.4664 - val_loss: 0.3594 - val_main_output_loss: 0.3480 - val_aux_output_loss: 0.4619\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=20, validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3452 - main_output_loss: 0.3294 - aux_output_loss: 0.4876\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Model using Subclass API\n",
    "\n",
    "*Allows building complex dynamic models, with loops, conditional branching etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideandDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Creating Layers (Not Calling them rn) (creation)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1, name='main_output')\n",
    "        self.aux_output = keras.layers.Dense(1, name='aux_output')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputA, inputB = inputs  # Extracting Inputs from Input Tuple\n",
    "        \n",
    "        # Calling Layers (usage)\n",
    "        hidden1 = self.hidden1(inputB)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate((inputA, inputB))\n",
    "        output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideandDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pro of Subclass API*\n",
    "* Allows to create complex architecture within the model\n",
    "\n",
    "*Con of Subclass API*\n",
    "* Keras has very little information about the model, it cannot save, clone or show proper summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring a Model\n",
    "\n",
    "*Works with Functional and Sequential API based models, for Subclass API `save_weights()` and `load_weights()` can be used to save the weights*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_keras_mode.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras stores model architecture as well as wights and biases in `HDF5` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "*Keras calls the callback objects at the start and end of each epoch or processing each batch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Checkpoint: Save Model after Regular Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 789us/step - loss: 1.4467 - main_output_loss: 1.2291 - aux_output_loss: 3.40490s - loss: 1.6529 - main_output_loss: 1.4044 - aux_output_loss: 3.\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 830us/step - loss: 0.9001 - main_output_loss: 0.8635 - aux_output_loss: 1.2300\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 764us/step - loss: 0.5134 - main_output_loss: 0.4654 - aux_output_loss: 0.9455\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.4715 - main_output_loss: 0.4352 - aux_output_loss: 0.7982\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.4314 - main_output_loss: 0.4015 - aux_output_loss: 0.7005\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4272 - main_output_loss: 0.4019 - aux_output_loss: 0.6550\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.4096 - main_output_loss: 0.3853 - aux_output_loss: 0.6281\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 783us/step - loss: 0.4329 - main_output_loss: 0.4107 - aux_output_loss: 0.6330\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 897us/step - loss: 0.3994 - main_output_loss: 0.3797 - aux_output_loss: 0.5766\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.4123 - main_output_loss: 0.3914 - aux_output_loss: 0.6003\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Best Model: Saves model only when performance on Validation Set is best so far (i.e. protects from overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('my_keras_model.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3991 - main_output_loss: 0.3789 - aux_output_loss: 0.5811 - val_loss: 0.4129 - val_main_output_loss: 0.3972 - val_aux_output_loss: 0.5536\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4000 - main_output_loss: 0.3803 - aux_output_loss: 0.5772 - val_loss: 0.3915 - val_main_output_loss: 0.3742 - val_aux_output_loss: 0.5469\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3945 - main_output_loss: 0.3747 - aux_output_loss: 0.5733 - val_loss: 0.3867 - val_main_output_loss: 0.3693 - val_aux_output_loss: 0.5436\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3901 - main_output_loss: 0.3703 - aux_output_loss: 0.5686 - val_loss: 0.3897 - val_main_output_loss: 0.3728 - val_aux_output_loss: 0.5417\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3872 - main_output_loss: 0.3675 - aux_output_loss: 0.5645 - val_loss: 0.3880 - val_main_output_loss: 0.3704 - val_aux_output_loss: 0.5455\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3852 - main_output_loss: 0.3655 - aux_output_loss: 0.5620 - val_loss: 0.3846 - val_main_output_loss: 0.3677 - val_aux_output_loss: 0.5362\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3812 - main_output_loss: 0.3616 - aux_output_loss: 0.5581 - val_loss: 0.3730 - val_main_output_loss: 0.3556 - val_aux_output_loss: 0.5300\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3765 - main_output_loss: 0.3573 - aux_output_loss: 0.5499 - val_loss: 0.3776 - val_main_output_loss: 0.3606 - val_aux_output_loss: 0.5310\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3765 - main_output_loss: 0.3576 - aux_output_loss: 0.5467 - val_loss: 0.3718 - val_main_output_loss: 0.3549 - val_aux_output_loss: 0.5239\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3706 - main_output_loss: 0.3516 - aux_output_loss: 0.5416 - val_loss: 0.3640 - val_main_output_loss: 0.3471 - val_aux_output_loss: 0.5160\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)), epochs=10, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll Back to Best Model\n",
    "\n",
    "model = keras.models.load_model('my_keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping: Stops Training if no significant change in accuracy of validation set is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3465 - main_output_loss: 0.3311 - aux_output_loss: 0.4855 - val_loss: 0.3401 - val_main_output_loss: 0.3265 - val_aux_output_loss: 0.4630\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3463 - main_output_loss: 0.3314 - aux_output_loss: 0.4807 - val_loss: 0.3475 - val_main_output_loss: 0.3348 - val_aux_output_loss: 0.4622\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3428 - main_output_loss: 0.3281 - aux_output_loss: 0.4748 - val_loss: 0.3651 - val_main_output_loss: 0.3524 - val_aux_output_loss: 0.4794\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3415 - main_output_loss: 0.3271 - aux_output_loss: 0.4711 - val_loss: 0.3415 - val_main_output_loss: 0.3289 - val_aux_output_loss: 0.4547\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3407 - main_output_loss: 0.3267 - aux_output_loss: 0.4666 - val_loss: 0.3348 - val_main_output_loss: 0.3224 - val_aux_output_loss: 0.4459\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3398 - main_output_loss: 0.3261 - aux_output_loss: 0.4633 - val_loss: 0.3346 - val_main_output_loss: 0.3222 - val_aux_output_loss: 0.4454\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3375 - main_output_loss: 0.3240 - aux_output_loss: 0.4586 - val_loss: 0.3341 - val_main_output_loss: 0.3223 - val_aux_output_loss: 0.4400\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3374 - main_output_loss: 0.3243 - aux_output_loss: 0.4554 - val_loss: 0.3333 - val_main_output_loss: 0.3218 - val_aux_output_loss: 0.4369\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3338 - main_output_loss: 0.3210 - aux_output_loss: 0.4495 - val_loss: 0.3402 - val_main_output_loss: 0.3291 - val_aux_output_loss: 0.4399\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3335 - main_output_loss: 0.3210 - aux_output_loss: 0.4457 - val_loss: 0.3357 - val_main_output_loss: 0.3246 - val_aux_output_loss: 0.4356\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3331 - main_output_loss: 0.3209 - aux_output_loss: 0.4427 - val_loss: 0.3323 - val_main_output_loss: 0.3217 - val_aux_output_loss: 0.4279\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3304 - main_output_loss: 0.3182 - aux_output_loss: 0.4395 - val_loss: 0.3342 - val_main_output_loss: 0.3235 - val_aux_output_loss: 0.4308\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3304 - main_output_loss: 0.3186 - aux_output_loss: 0.4363 - val_loss: 0.3277 - val_main_output_loss: 0.3168 - val_aux_output_loss: 0.4258\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3288 - main_output_loss: 0.3172 - aux_output_loss: 0.4337 - val_loss: 0.3306 - val_main_output_loss: 0.3200 - val_aux_output_loss: 0.4260\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3285 - main_output_loss: 0.3171 - aux_output_loss: 0.4312 - val_loss: 0.3283 - val_main_output_loss: 0.3182 - val_aux_output_loss: 0.4192\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3283 - main_output_loss: 0.3170 - aux_output_loss: 0.4298 - val_loss: 0.3354 - val_main_output_loss: 0.3258 - val_aux_output_loss: 0.4216\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3267 - main_output_loss: 0.3159 - aux_output_loss: 0.4237 - val_loss: 0.3264 - val_main_output_loss: 0.3169 - val_aux_output_loss: 0.4114\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - main_output_loss: 0.3154 - aux_output_loss: 0.4226 - val_loss: 0.3231 - val_main_output_loss: 0.3132 - val_aux_output_loss: 0.4120\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3249 - main_output_loss: 0.3143 - aux_output_loss: 0.4209 - val_loss: 0.3320 - val_main_output_loss: 0.3230 - val_aux_output_loss: 0.4132\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3238 - main_output_loss: 0.3133 - aux_output_loss: 0.4179 - val_loss: 0.3194 - val_main_output_loss: 0.3099 - val_aux_output_loss: 0.4044\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3235 - main_output_loss: 0.3132 - aux_output_loss: 0.4163 - val_loss: 0.3210 - val_main_output_loss: 0.3115 - val_aux_output_loss: 0.4058\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3228 - main_output_loss: 0.3126 - aux_output_loss: 0.4147 - val_loss: 0.3222 - val_main_output_loss: 0.3132 - val_aux_output_loss: 0.4029\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3220 - main_output_loss: 0.3120 - aux_output_loss: 0.4119 - val_loss: 0.3261 - val_main_output_loss: 0.3175 - val_aux_output_loss: 0.4039\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3215 - main_output_loss: 0.3117 - aux_output_loss: 0.4105 - val_loss: 0.3179 - val_main_output_loss: 0.3090 - val_aux_output_loss: 0.3973\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3209 - main_output_loss: 0.3110 - aux_output_loss: 0.4101 - val_loss: 0.3233 - val_main_output_loss: 0.3144 - val_aux_output_loss: 0.4031\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3194 - main_output_loss: 0.3097 - aux_output_loss: 0.4065 - val_loss: 0.3219 - val_main_output_loss: 0.3132 - val_aux_output_loss: 0.3994\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3196 - main_output_loss: 0.3102 - aux_output_loss: 0.4043 - val_loss: 0.3178 - val_main_output_loss: 0.3090 - val_aux_output_loss: 0.3974\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3255 - main_output_loss: 0.3167 - aux_output_loss: 0.4043 - val_loss: 0.3313 - val_main_output_loss: 0.3234 - val_aux_output_loss: 0.4024\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3178 - main_output_loss: 0.3084 - aux_output_loss: 0.4022 - val_loss: 0.3187 - val_main_output_loss: 0.3104 - val_aux_output_loss: 0.3935\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3194 - main_output_loss: 0.3104 - aux_output_loss: 0.4002 - val_loss: 0.3287 - val_main_output_loss: 0.3216 - val_aux_output_loss: 0.3927\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3173 - main_output_loss: 0.3084 - aux_output_loss: 0.3975 - val_loss: 0.3184 - val_main_output_loss: 0.3103 - val_aux_output_loss: 0.3916\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3152 - main_output_loss: 0.3060 - aux_output_loss: 0.3982 - val_loss: 0.3227 - val_main_output_loss: 0.3144 - val_aux_output_loss: 0.3969\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3153 - main_output_loss: 0.3064 - aux_output_loss: 0.3953 - val_loss: 0.3210 - val_main_output_loss: 0.3128 - val_aux_output_loss: 0.3948\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3160 - main_output_loss: 0.3074 - aux_output_loss: 0.3935 - val_loss: 0.3262 - val_main_output_loss: 0.3186 - val_aux_output_loss: 0.3948\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3141 - main_output_loss: 0.3054 - aux_output_loss: 0.3929 - val_loss: 0.3150 - val_main_output_loss: 0.3070 - val_aux_output_loss: 0.3867\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3144 - main_output_loss: 0.3058 - aux_output_loss: 0.3924 - val_loss: 0.3197 - val_main_output_loss: 0.3122 - val_aux_output_loss: 0.3866\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3131 - main_output_loss: 0.3044 - aux_output_loss: 0.3912 - val_loss: 0.3164 - val_main_output_loss: 0.3088 - val_aux_output_loss: 0.3853\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3121 - main_output_loss: 0.3036 - aux_output_loss: 0.3889 - val_loss: 0.3230 - val_main_output_loss: 0.3159 - val_aux_output_loss: 0.3872\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3121 - main_output_loss: 0.3037 - aux_output_loss: 0.3879 - val_loss: 0.3159 - val_main_output_loss: 0.3081 - val_aux_output_loss: 0.3863\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3129 - main_output_loss: 0.3044 - aux_output_loss: 0.3887 - val_loss: 0.3180 - val_main_output_loss: 0.3109 - val_aux_output_loss: 0.3828\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3111 - main_output_loss: 0.3027 - aux_output_loss: 0.3869 - val_loss: 0.3137 - val_main_output_loss: 0.3060 - val_aux_output_loss: 0.3822\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3102 - main_output_loss: 0.3019 - aux_output_loss: 0.3851 - val_loss: 0.3162 - val_main_output_loss: 0.3085 - val_aux_output_loss: 0.3858\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3101 - main_output_loss: 0.3017 - aux_output_loss: 0.3853 - val_loss: 0.3140 - val_main_output_loss: 0.3064 - val_aux_output_loss: 0.3823\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3106 - main_output_loss: 0.3024 - aux_output_loss: 0.3843 - val_loss: 0.3162 - val_main_output_loss: 0.3086 - val_aux_output_loss: 0.3848\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3099 - main_output_loss: 0.3018 - aux_output_loss: 0.3829 - val_loss: 0.3546 - val_main_output_loss: 0.3477 - val_aux_output_loss: 0.4160\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3101 - main_output_loss: 0.3020 - aux_output_loss: 0.3832 - val_loss: 0.3149 - val_main_output_loss: 0.3075 - val_aux_output_loss: 0.3820\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3122 - main_output_loss: 0.3044 - aux_output_loss: 0.3822 - val_loss: 0.3151 - val_main_output_loss: 0.3082 - val_aux_output_loss: 0.3768\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3095 - main_output_loss: 0.3016 - aux_output_loss: 0.3803 - val_loss: 0.3091 - val_main_output_loss: 0.3018 - val_aux_output_loss: 0.3756\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3085 - main_output_loss: 0.3007 - aux_output_loss: 0.3791 - val_loss: 0.3095 - val_main_output_loss: 0.3023 - val_aux_output_loss: 0.3737\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3061 - main_output_loss: 0.2981 - aux_output_loss: 0.3777 - val_loss: 0.3158 - val_main_output_loss: 0.3085 - val_aux_output_loss: 0.3813\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3091 - main_output_loss: 0.3015 - aux_output_loss: 0.3779 - val_loss: 0.3076 - val_main_output_loss: 0.3003 - val_aux_output_loss: 0.3733\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3064 - main_output_loss: 0.2987 - aux_output_loss: 0.3758 - val_loss: 0.3080 - val_main_output_loss: 0.3007 - val_aux_output_loss: 0.3735\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3070 - main_output_loss: 0.2993 - aux_output_loss: 0.3764 - val_loss: 0.3098 - val_main_output_loss: 0.3027 - val_aux_output_loss: 0.3734\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3059 - main_output_loss: 0.2982 - aux_output_loss: 0.3747 - val_loss: 0.3197 - val_main_output_loss: 0.3135 - val_aux_output_loss: 0.3759\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3057 - main_output_loss: 0.2980 - aux_output_loss: 0.3752 - val_loss: 0.3059 - val_main_output_loss: 0.2988 - val_aux_output_loss: 0.3699\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3065 - main_output_loss: 0.2989 - aux_output_loss: 0.3745 - val_loss: 0.3057 - val_main_output_loss: 0.2984 - val_aux_output_loss: 0.3707\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3063 - main_output_loss: 0.2987 - aux_output_loss: 0.3749 - val_loss: 0.3066 - val_main_output_loss: 0.2998 - val_aux_output_loss: 0.3674\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3048 - main_output_loss: 0.2974 - aux_output_loss: 0.3721 - val_loss: 0.3098 - val_main_output_loss: 0.3032 - val_aux_output_loss: 0.3694\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3045 - main_output_loss: 0.2971 - aux_output_loss: 0.3710 - val_loss: 0.3055 - val_main_output_loss: 0.2981 - val_aux_output_loss: 0.3720\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3045 - main_output_loss: 0.2971 - aux_output_loss: 0.3717 - val_loss: 0.3112 - val_main_output_loss: 0.3041 - val_aux_output_loss: 0.3751\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3030 - main_output_loss: 0.2957 - aux_output_loss: 0.3691 - val_loss: 0.3076 - val_main_output_loss: 0.3010 - val_aux_output_loss: 0.3672\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3042 - main_output_loss: 0.2969 - aux_output_loss: 0.3697 - val_loss: 0.3066 - val_main_output_loss: 0.2997 - val_aux_output_loss: 0.3690\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3039 - main_output_loss: 0.2965 - aux_output_loss: 0.3712 - val_loss: 0.3027 - val_main_output_loss: 0.2957 - val_aux_output_loss: 0.3654\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3027 - main_output_loss: 0.2954 - aux_output_loss: 0.3682 - val_loss: 0.3110 - val_main_output_loss: 0.3044 - val_aux_output_loss: 0.3697\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3042 - main_output_loss: 0.2970 - aux_output_loss: 0.3687 - val_loss: 0.3339 - val_main_output_loss: 0.3278 - val_aux_output_loss: 0.3891\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3017 - main_output_loss: 0.2944 - aux_output_loss: 0.3671 - val_loss: 0.3222 - val_main_output_loss: 0.3155 - val_aux_output_loss: 0.3819\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3003 - main_output_loss: 0.2932 - aux_output_loss: 0.3645 - val_loss: 0.3143 - val_main_output_loss: 0.3071 - val_aux_output_loss: 0.3790\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3024 - main_output_loss: 0.2952 - aux_output_loss: 0.3665 - val_loss: 0.3088 - val_main_output_loss: 0.3020 - val_aux_output_loss: 0.3698\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3015 - main_output_loss: 0.2943 - aux_output_loss: 0.3663 - val_loss: 0.3020 - val_main_output_loss: 0.2951 - val_aux_output_loss: 0.3643\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3008 - main_output_loss: 0.2936 - aux_output_loss: 0.3664 - val_loss: 0.3074 - val_main_output_loss: 0.3007 - val_aux_output_loss: 0.3682\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3021 - main_output_loss: 0.2950 - aux_output_loss: 0.3658 - val_loss: 0.3037 - val_main_output_loss: 0.2969 - val_aux_output_loss: 0.3646\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3010 - main_output_loss: 0.2939 - aux_output_loss: 0.3644 - val_loss: 0.3017 - val_main_output_loss: 0.2949 - val_aux_output_loss: 0.3628\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3004 - main_output_loss: 0.2932 - aux_output_loss: 0.3650 - val_loss: 0.3070 - val_main_output_loss: 0.3002 - val_aux_output_loss: 0.3684\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2993 - main_output_loss: 0.2921 - aux_output_loss: 0.3635 - val_loss: 0.3039 - val_main_output_loss: 0.2975 - val_aux_output_loss: 0.3616\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2991 - main_output_loss: 0.2920 - aux_output_loss: 0.3629 - val_loss: 0.3106 - val_main_output_loss: 0.3038 - val_aux_output_loss: 0.3716\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2982 - main_output_loss: 0.2912 - aux_output_loss: 0.3616 - val_loss: 0.3032 - val_main_output_loss: 0.2966 - val_aux_output_loss: 0.3629\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2985 - main_output_loss: 0.2915 - aux_output_loss: 0.3622 - val_loss: 0.3132 - val_main_output_loss: 0.3066 - val_aux_output_loss: 0.3721\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2990 - main_output_loss: 0.2920 - aux_output_loss: 0.3622 - val_loss: 0.3032 - val_main_output_loss: 0.2966 - val_aux_output_loss: 0.3629\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2988 - main_output_loss: 0.2917 - aux_output_loss: 0.3620 - val_loss: 0.3052 - val_main_output_loss: 0.2983 - val_aux_output_loss: 0.3665\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2978 - main_output_loss: 0.2908 - aux_output_loss: 0.3613 - val_loss: 0.3009 - val_main_output_loss: 0.2944 - val_aux_output_loss: 0.3601\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2980 - main_output_loss: 0.2910 - aux_output_loss: 0.3606 - val_loss: 0.3013 - val_main_output_loss: 0.2947 - val_aux_output_loss: 0.3605\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2966 - main_output_loss: 0.2897 - aux_output_loss: 0.3585 - val_loss: 0.3023 - val_main_output_loss: 0.2957 - val_aux_output_loss: 0.3614\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2979 - main_output_loss: 0.2910 - aux_output_loss: 0.3603 - val_loss: 0.3033 - val_main_output_loss: 0.2969 - val_aux_output_loss: 0.3607\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2959 - main_output_loss: 0.2889 - aux_output_loss: 0.3585 - val_loss: 0.3088 - val_main_output_loss: 0.3029 - val_aux_output_loss: 0.3618\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2963 - main_output_loss: 0.2894 - aux_output_loss: 0.3590 - val_loss: 0.3082 - val_main_output_loss: 0.3017 - val_aux_output_loss: 0.3666\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2965 - main_output_loss: 0.2896 - aux_output_loss: 0.3585 - val_loss: 0.3086 - val_main_output_loss: 0.3018 - val_aux_output_loss: 0.3694\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2969 - main_output_loss: 0.2899 - aux_output_loss: 0.3593 - val_loss: 0.3368 - val_main_output_loss: 0.3320 - val_aux_output_loss: 0.3799\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2978 - main_output_loss: 0.2909 - aux_output_loss: 0.3599 - val_loss: 0.3024 - val_main_output_loss: 0.2961 - val_aux_output_loss: 0.3588\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2968 - main_output_loss: 0.2900 - aux_output_loss: 0.3578 - val_loss: 0.3088 - val_main_output_loss: 0.3025 - val_aux_output_loss: 0.3650\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2968 - main_output_loss: 0.2900 - aux_output_loss: 0.3580 - val_loss: 0.3083 - val_main_output_loss: 0.3019 - val_aux_output_loss: 0.3658\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)), epochs=100, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print('\\nval/train: {:.2f}'.format(logs['val_loss']/logs['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Available Methods are `on_train_begin`, `on_train_end`, `on_epoch_begin`, `on_epoch_end`, `on_batch_begin` and `on_batch_end`.\n",
    "\n",
    "**Note: Callbacks can also be used during Evaluation and Prediction**\n",
    "\n",
    "Callbacks for evaluation `on_test_begin`, `on_test_end`, `on_test_batch_begin` and `on_test_batch_end`.\n",
    "\n",
    "Callbacks for prediction `on_predict_begin`, `on_predict_end`, `on_predict_batch_begin` and `on_predict_batch_end`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "*Tensorflow writes log flies (event files), while training to a root directory. Tensorboard reades these files from the root directory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, 'my_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = get_run_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./my_logs/run_2021_05_03-02_16_00'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras Tensorboard Callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2971 - main_output_loss: 0.2902 - aux_output_loss: 0.3593 - val_loss: 0.3016 - val_main_output_loss: 0.2950 - val_aux_output_loss: 0.3605\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2973 - main_output_loss: 0.2903 - aux_output_loss: 0.3603 - val_loss: 0.3030 - val_main_output_loss: 0.2963 - val_aux_output_loss: 0.3628\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2994 - main_output_loss: 0.2926 - aux_output_loss: 0.3605 - val_loss: 0.3055 - val_main_output_loss: 0.2990 - val_aux_output_loss: 0.3640\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2964 - main_output_loss: 0.2894 - aux_output_loss: 0.3593 - val_loss: 0.3059 - val_main_output_loss: 0.2995 - val_aux_output_loss: 0.3640\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2971 - main_output_loss: 0.2901 - aux_output_loss: 0.3597 - val_loss: 0.3078 - val_main_output_loss: 0.3016 - val_aux_output_loss: 0.3631\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2972 - main_output_loss: 0.2903 - aux_output_loss: 0.3592 - val_loss: 0.3032 - val_main_output_loss: 0.2965 - val_aux_output_loss: 0.3633\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2968 - main_output_loss: 0.2899 - aux_output_loss: 0.3592 - val_loss: 0.3020 - val_main_output_loss: 0.2956 - val_aux_output_loss: 0.3595\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2972 - main_output_loss: 0.2905 - aux_output_loss: 0.3570 - val_loss: 0.2998 - val_main_output_loss: 0.2933 - val_aux_output_loss: 0.3585\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2958 - main_output_loss: 0.2889 - aux_output_loss: 0.3574 - val_loss: 0.3074 - val_main_output_loss: 0.3009 - val_aux_output_loss: 0.3654\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2963 - main_output_loss: 0.2895 - aux_output_loss: 0.3572 - val_loss: 0.3129 - val_main_output_loss: 0.3063 - val_aux_output_loss: 0.3723\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2969 - main_output_loss: 0.2900 - aux_output_loss: 0.3584 - val_loss: 0.3086 - val_main_output_loss: 0.3024 - val_aux_output_loss: 0.3640\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2951 - main_output_loss: 0.2882 - aux_output_loss: 0.3571 - val_loss: 0.3101 - val_main_output_loss: 0.3038 - val_aux_output_loss: 0.3672\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2947 - main_output_loss: 0.2879 - aux_output_loss: 0.3556 - val_loss: 0.3012 - val_main_output_loss: 0.2947 - val_aux_output_loss: 0.3595\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2955 - main_output_loss: 0.2887 - aux_output_loss: 0.3564 - val_loss: 0.3300 - val_main_output_loss: 0.3238 - val_aux_output_loss: 0.3857\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2942 - main_output_loss: 0.2873 - aux_output_loss: 0.3563 - val_loss: 0.2977 - val_main_output_loss: 0.2914 - val_aux_output_loss: 0.3549\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2952 - main_output_loss: 0.2884 - aux_output_loss: 0.3563 - val_loss: 0.3025 - val_main_output_loss: 0.2963 - val_aux_output_loss: 0.3583\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2940 - main_output_loss: 0.2872 - aux_output_loss: 0.3558 - val_loss: 0.3004 - val_main_output_loss: 0.2941 - val_aux_output_loss: 0.3572\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2942 - main_output_loss: 0.2874 - aux_output_loss: 0.3556 - val_loss: 0.2988 - val_main_output_loss: 0.2926 - val_aux_output_loss: 0.3546\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2953 - main_output_loss: 0.2887 - aux_output_loss: 0.3552 - val_loss: 0.2983 - val_main_output_loss: 0.2921 - val_aux_output_loss: 0.3543\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2949 - main_output_loss: 0.2883 - aux_output_loss: 0.3545 - val_loss: 0.3004 - val_main_output_loss: 0.2943 - val_aux_output_loss: 0.3551\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2943 - main_output_loss: 0.2877 - aux_output_loss: 0.3541 - val_loss: 0.3003 - val_main_output_loss: 0.2942 - val_aux_output_loss: 0.3554\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2937 - main_output_loss: 0.2870 - aux_output_loss: 0.3546 - val_loss: 0.2968 - val_main_output_loss: 0.2903 - val_aux_output_loss: 0.3546\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2950 - main_output_loss: 0.2886 - aux_output_loss: 0.3523 - val_loss: 0.3006 - val_main_output_loss: 0.2945 - val_aux_output_loss: 0.3554\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2940 - main_output_loss: 0.2873 - aux_output_loss: 0.3541 - val_loss: 0.3053 - val_main_output_loss: 0.2991 - val_aux_output_loss: 0.3605\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2923 - main_output_loss: 0.2855 - aux_output_loss: 0.3535 - val_loss: 0.3040 - val_main_output_loss: 0.2976 - val_aux_output_loss: 0.3616\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2918 - main_output_loss: 0.2851 - aux_output_loss: 0.3515 - val_loss: 0.3089 - val_main_output_loss: 0.3032 - val_aux_output_loss: 0.3601\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2936 - main_output_loss: 0.2870 - aux_output_loss: 0.3530 - val_loss: 0.3092 - val_main_output_loss: 0.3035 - val_aux_output_loss: 0.3610\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.2929 - main_output_loss: 0.2862 - aux_output_loss: 0.3529 - val_loss: 0.3067 - val_main_output_loss: 0.3006 - val_aux_output_loss: 0.3612\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2926 - main_output_loss: 0.2860 - aux_output_loss: 0.3525 - val_loss: 0.3134 - val_main_output_loss: 0.3083 - val_aux_output_loss: 0.3595\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.2922 - main_output_loss: 0.2855 - aux_output_loss: 0.3524 - val_loss: 0.3151 - val_main_output_loss: 0.3091 - val_aux_output_loss: 0.3691\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)), epochs=30, \n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Launch Tensorboard from Command Line: `tensorboard --logdir=./my_logs --port=6006`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Launch Tensoboard within Jupyter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eceb413d8df24e53\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eceb413d8df24e53\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SummaryWriter**\n",
    "\n",
    "Custom Data Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000+1):\n",
    "        tf.summary.scalar('my_scalar', np.sin(step/10), step=step)\n",
    "        data = (np.random.randn(100)+2)* step/100. # Creating Random Data\n",
    "        tf.summary.histogram('my_hist', data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3)  # Creating Random 32x32 RGB Image\n",
    "        tf.summary.image('my_images', images*step/1000, step=step)\n",
    "        texts = ['This Step is ' + str(step), 'Its square is ' + str(step**2)]\n",
    "        tf.summary.text('my_text', texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(120000) / 48000 * 2 * np.pi *step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio('my_audio', audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuninig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Hyperparameter Tuning\n",
    "\n",
    "Using scikit learns' `GridSearchCV` or `RandomizedSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrapping a Keras Model to become Scikit Learn Compatible**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Univariate Regression\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now this model can be used like a normal Scikit Learn Model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.8138 - val_loss: 1.7483\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.3994 - val_loss: 0.6724\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6297 - val_loss: 0.5953\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 984us/step - loss: 0.5783 - val_loss: 0.5493\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 913us/step - loss: 0.5401 - val_loss: 0.5184\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.5012 - val_loss: 0.4955\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.4778 - val_loss: 0.4797\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.4453 - val_loss: 0.4702\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.4362 - val_loss: 0.4561\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 870us/step - loss: 0.4425 - val_loss: 0.4489\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 894us/step - loss: 0.4387 - val_loss: 0.4449\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.4266 - val_loss: 0.4375\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 866us/step - loss: 0.4084 - val_loss: 0.4342\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 947us/step - loss: 0.4083 - val_loss: 0.4288\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.4250 - val_loss: 0.4266\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.4165 - val_loss: 0.4228\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 885us/step - loss: 0.4007 - val_loss: 0.4186\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.4155\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 974us/step - loss: 0.4118 - val_loss: 0.4137\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4127\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.4083\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.4064\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.4042\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.4047\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.4001\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 817us/step - loss: 0.3875 - val_loss: 0.3980\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 896us/step - loss: 0.3926 - val_loss: 0.3967\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3914 - val_loss: 0.3947\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 997us/step - loss: 0.3841 - val_loss: 0.3942\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.3911 - val_loss: 0.3936\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3775 - val_loss: 0.3911\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 743us/step - loss: 0.3855 - val_loss: 0.3895\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3666 - val_loss: 0.3879\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 775us/step - loss: 0.3665 - val_loss: 0.3903\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.3585 - val_loss: 0.3862\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.3677 - val_loss: 0.3847\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 794us/step - loss: 0.3888 - val_loss: 0.3837\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.3806 - val_loss: 0.3809\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3671 - val_loss: 0.3796\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.3592 - val_loss: 0.3787\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.3742 - val_loss: 0.3792\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 796us/step - loss: 0.3656 - val_loss: 0.3783\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 877us/step - loss: 0.3617 - val_loss: 0.3758\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.3709 - val_loss: 0.3769\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.3716 - val_loss: 0.3726\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3549 - val_loss: 0.3753\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.3577 - val_loss: 0.3732\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.3528 - val_loss: 0.3714\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.3523 - val_loss: 0.3701\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.3668 - val_loss: 0.3697\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3609 - val_loss: 0.3683\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 795us/step - loss: 0.3616 - val_loss: 0.3688\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 838us/step - loss: 0.3599 - val_loss: 0.3655\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.3389 - val_loss: 0.3651\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.3607 - val_loss: 0.3660\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.3504 - val_loss: 0.3645\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 822us/step - loss: 0.3532 - val_loss: 0.3638\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.3376 - val_loss: 0.3629\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3659 - val_loss: 0.3614\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.3555 - val_loss: 0.3601\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 831us/step - loss: 0.3473 - val_loss: 0.3615\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.3528 - val_loss: 0.3593\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 797us/step - loss: 0.3537 - val_loss: 0.3612\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.3517 - val_loss: 0.3595\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3350 - val_loss: 0.3577\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 826us/step - loss: 0.3601 - val_loss: 0.3573\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.3524 - val_loss: 0.3580\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.3410 - val_loss: 0.3588\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.3418 - val_loss: 0.3568\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 795us/step - loss: 0.3470 - val_loss: 0.3551\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.3409 - val_loss: 0.3543\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.3319 - val_loss: 0.3552\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.3371 - val_loss: 0.3555\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.3469 - val_loss: 0.3520\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.3463 - val_loss: 0.3517\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3496 - val_loss: 0.3524\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 805us/step - loss: 0.3455 - val_loss: 0.3527\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 771us/step - loss: 0.3413 - val_loss: 0.3530\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.3399 - val_loss: 0.3517\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3217 - val_loss: 0.3554\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.3433 - val_loss: 0.3501\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.3418 - val_loss: 0.3482\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.3332 - val_loss: 0.3503\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3406 - val_loss: 0.3475\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.3437 - val_loss: 0.3477\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 960us/step - loss: 0.3259 - val_loss: 0.3476\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 890us/step - loss: 0.3208 - val_loss: 0.3469\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.3361 - val_loss: 0.3474\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 920us/step - loss: 0.3421 - val_loss: 0.3465\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3456\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 905us/step - loss: 0.3437 - val_loss: 0.3457\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 855us/step - loss: 0.3371 - val_loss: 0.3484\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 916us/step - loss: 0.3394 - val_loss: 0.3449\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.3416 - val_loss: 0.3449\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 957us/step - loss: 0.3333 - val_loss: 0.3454\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 916us/step - loss: 0.3375 - val_loss: 0.3444\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.3397 - val_loss: 0.3437\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 911us/step - loss: 0.3226 - val_loss: 0.3445\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 940us/step - loss: 0.3385 - val_loss: 0.3426\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 860us/step - loss: 0.3336 - val_loss: 0.3465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faafc15da00>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Any extra paramaters passed to fit, will be passed to underlying keras model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 797us/step - loss: 0.3291\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performing Randomized Search CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    'n_hidden': [0, 1, 2, 3],\n",
    "    'n_neurons': np.arange(1, 100),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.6688 - val_loss: 1.0745\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9571 - val_loss: 0.7755\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7440 - val_loss: 0.7119\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.6834 - val_loss: 0.6792\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6561 - val_loss: 0.6569\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6203 - val_loss: 0.6341\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6202 - val_loss: 0.6190\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5992 - val_loss: 0.6028\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5721 - val_loss: 0.5942\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5727 - val_loss: 0.5797\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5478 - val_loss: 0.5731\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5455 - val_loss: 0.5598\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5227 - val_loss: 0.5556\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.5845 - val_loss: 0.5436\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5313 - val_loss: 0.5383\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4995 - val_loss: 0.5304\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.4937 - val_loss: 0.5256\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5048 - val_loss: 0.5204\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4861 - val_loss: 0.5171\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4958 - val_loss: 0.5112\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4902 - val_loss: 0.5105\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4770 - val_loss: 0.5030\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4846 - val_loss: 0.5002\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4620 - val_loss: 0.4965\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4778 - val_loss: 0.4927\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4697 - val_loss: 0.4903\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.4639 - val_loss: 0.4887\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4459 - val_loss: 0.4850\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4819\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.4810\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4631 - val_loss: 0.4769\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4709 - val_loss: 0.4772\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4514 - val_loss: 0.4736\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4527 - val_loss: 0.4715\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4333 - val_loss: 0.4689\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4665\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4646\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4635\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4396 - val_loss: 0.4615\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4594\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4636 - val_loss: 0.4572\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4161 - val_loss: 0.4569\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4476 - val_loss: 0.4540\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4424 - val_loss: 0.4522\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4308 - val_loss: 0.4508\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4510\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4480\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4174 - val_loss: 0.4481\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4458\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3943 - val_loss: 0.4441\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.4435\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4152 - val_loss: 0.4425\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4165 - val_loss: 0.4409\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.4294 - val_loss: 0.4396\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.4066 - val_loss: 0.4387\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.4064 - val_loss: 0.4379\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 0.4368\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4398 - val_loss: 0.4360\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.4342\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.4343\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3949 - val_loss: 0.4331\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4222 - val_loss: 0.4317\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.4299\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 0.4287\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3879 - val_loss: 0.4291\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4194 - val_loss: 0.4269\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.4270\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4254\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3932 - val_loss: 0.4257\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.4239\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4015 - val_loss: 0.4247\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4247 - val_loss: 0.4221\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.4219\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4005 - val_loss: 0.4205\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.4205\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4064 - val_loss: 0.4191\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.4184\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.4186\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3870 - val_loss: 0.4174\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3917 - val_loss: 0.4162\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.4149\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4024 - val_loss: 0.4156\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 0.4143\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.4141\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.4096 - val_loss: 0.4123\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4126 - val_loss: 0.4122\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3855 - val_loss: 0.4122\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4104\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3823 - val_loss: 0.4095\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3973 - val_loss: 0.4091\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3840 - val_loss: 0.4095\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.4074\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.4089\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.4069\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4065\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.4056\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3894 - val_loss: 0.4060\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3904 - val_loss: 0.4049\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3852 - val_loss: 0.4046\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3753 - val_loss: 0.4035\n",
      "121/121 [==============================] - 0s 547us/step - loss: 0.3995\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.4169 - val_loss: 0.9858\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9151 - val_loss: 0.7579\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6903 - val_loss: 0.7168\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7036 - val_loss: 0.6899\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6639 - val_loss: 0.6704\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6771 - val_loss: 0.6506\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6393 - val_loss: 0.6336\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6350 - val_loss: 0.6197\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6177 - val_loss: 0.6049\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6016 - val_loss: 0.5916\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5659 - val_loss: 0.5800\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5712 - val_loss: 0.5695\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5542 - val_loss: 0.5592\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5533 - val_loss: 0.5504\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5460 - val_loss: 0.5431\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.5353\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5293 - val_loss: 0.5282\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5187 - val_loss: 0.5217\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4893 - val_loss: 0.5155\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.5124 - val_loss: 0.5104\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.5051 - val_loss: 0.5052\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4929 - val_loss: 0.5010\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.5069 - val_loss: 0.4966\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.4677 - val_loss: 0.4923\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4833 - val_loss: 0.4889\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4755 - val_loss: 0.4852\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4481 - val_loss: 0.4826\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4551 - val_loss: 0.4785\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.4585 - val_loss: 0.4755\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4811 - val_loss: 0.4734\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4617 - val_loss: 0.4708\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4557 - val_loss: 0.4683\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.4638 - val_loss: 0.4656\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4638 - val_loss: 0.4634\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4357 - val_loss: 0.4612\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4451 - val_loss: 0.4591\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4374 - val_loss: 0.4574\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.4313 - val_loss: 0.4556\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4157 - val_loss: 0.4542\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4322 - val_loss: 0.4518\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.4355 - val_loss: 0.4503\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4313 - val_loss: 0.4487\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.4348 - val_loss: 0.4469\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4287 - val_loss: 0.4452\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4238 - val_loss: 0.4447\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.4171 - val_loss: 0.4428\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4256 - val_loss: 0.4413\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.4371 - val_loss: 0.4402\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4138 - val_loss: 0.4377\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.4383 - val_loss: 0.4368\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4382 - val_loss: 0.4352\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4375 - val_loss: 0.4344\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4204 - val_loss: 0.4336\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4127 - val_loss: 0.4327\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3992 - val_loss: 0.4311\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.4305 - val_loss: 0.4299\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4228 - val_loss: 0.4290\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4234 - val_loss: 0.4279\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4253 - val_loss: 0.4264\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4127 - val_loss: 0.4259\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3999 - val_loss: 0.4254\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4078 - val_loss: 0.4240\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3940 - val_loss: 0.4232\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4254 - val_loss: 0.4218\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4020 - val_loss: 0.4217\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4000 - val_loss: 0.4203\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3994 - val_loss: 0.4203\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3962 - val_loss: 0.4190\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3864 - val_loss: 0.4174\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3975 - val_loss: 0.4172\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4038 - val_loss: 0.4163\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4024 - val_loss: 0.4155\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4025 - val_loss: 0.4147\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4199 - val_loss: 0.4136\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4171 - val_loss: 0.4132\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4027 - val_loss: 0.4127\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3905 - val_loss: 0.4117\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3844 - val_loss: 0.4110\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3905 - val_loss: 0.4104\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4035 - val_loss: 0.4097\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3997 - val_loss: 0.4088\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3886 - val_loss: 0.4086\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3871 - val_loss: 0.4085\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3958 - val_loss: 0.4083\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3743 - val_loss: 0.4064\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4024 - val_loss: 0.4066\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3895 - val_loss: 0.4053\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3950 - val_loss: 0.4053\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3847 - val_loss: 0.4040\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3919 - val_loss: 0.4045\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4006 - val_loss: 0.4035\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3860 - val_loss: 0.4022\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3968 - val_loss: 0.4017\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3919 - val_loss: 0.4013\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3669 - val_loss: 0.4017\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3820 - val_loss: 0.4005\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3974 - val_loss: 0.4000\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3920 - val_loss: 0.3996\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3902 - val_loss: 0.3988\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3982 - val_loss: 0.3981\n",
      "121/121 [==============================] - 0s 498us/step - loss: 0.3910\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4295 - val_loss: 1.0017\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.9296 - val_loss: 0.8167\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.8216 - val_loss: 0.7670\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.7709 - val_loss: 0.7328\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.7261 - val_loss: 0.7058\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.7202 - val_loss: 0.6832\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.6738 - val_loss: 0.6648\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.6617 - val_loss: 0.6497\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.6121 - val_loss: 0.6379\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.6103 - val_loss: 0.6289\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.5694 - val_loss: 0.6215\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5802 - val_loss: 0.6176\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.5623 - val_loss: 0.6150\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.5457 - val_loss: 0.6127\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.5129 - val_loss: 0.6115\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5033 - val_loss: 0.6133\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.5160 - val_loss: 0.6138\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.5196 - val_loss: 0.6147\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.5083 - val_loss: 0.6152\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4849 - val_loss: 0.6165\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4790 - val_loss: 0.6188\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4680 - val_loss: 0.6197\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4782 - val_loss: 0.6218\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4791 - val_loss: 0.6233\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4635 - val_loss: 0.6223\n",
      "121/121 [==============================] - 0s 497us/step - loss: 0.6315\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.8666 - val_loss: 16.9090\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 6.3944 - val_loss: 440.8144\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 604.5881 - val_loss: 10895.0723\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 3706.2608 - val_loss: 271584.4062\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 222600.2241 - val_loss: 6720622.0000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 748349.2874 - val_loss: 170744192.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 138170010.7212 - val_loss: 4207686144.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 364337981.7778 - val_loss: 104901124096.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 434976358327.8354 - val_loss: 2595462053888.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 2103012330858.4033 - val_loss: 64409577717760.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 58697699321662.1562 - val_loss: 1590439140392960.0000\n",
      "121/121 [==============================] - 0s 482us/step - loss: 31011131883520.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 5.7898 - val_loss: 35.5965\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 10.4039 - val_loss: 990.8049\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 5063.5739 - val_loss: 29786.3789\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 6273.5166 - val_loss: 900856.3750\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 1805781.4919 - val_loss: 27152194.0000\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 6869604.6211 - val_loss: 822244352.0000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 1076939683.3477 - val_loss: 24783298560.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 7710459804.0494 - val_loss: 749542637568.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 77956279390.8148 - val_loss: 22764121489408.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 29489644767678.6836 - val_loss: 683322921451520.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 291181615286516.4375 - val_loss: 20692830309580800.0000\n",
      "121/121 [==============================] - 0s 478us/step - loss: 729929893281792.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0466 - val_loss: 0.6253\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.6188 - val_loss: 0.7060\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.5507 - val_loss: 0.8431\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5234 - val_loss: 0.9392\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.5275 - val_loss: 1.0276\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.5328 - val_loss: 1.1135\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.5250 - val_loss: 1.1532\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.5025 - val_loss: 1.1645\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5184 - val_loss: 1.2256\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.5030 - val_loss: 1.2150\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.5013 - val_loss: 1.1927\n",
      "121/121 [==============================] - 0s 516us/step - loss: 1.4302\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4983 - val_loss: 1.3148\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 1.2014 - val_loss: 0.9284\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.8404 - val_loss: 0.7981\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.7478 - val_loss: 0.7220\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.6879 - val_loss: 0.6796\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.6881 - val_loss: 0.6425\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.6145 - val_loss: 0.6136\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5979 - val_loss: 0.5888\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5763 - val_loss: 0.5713\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5482 - val_loss: 0.5551\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5402 - val_loss: 0.5409\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5085 - val_loss: 0.5285\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5149 - val_loss: 0.5193\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4964 - val_loss: 0.5110\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4891 - val_loss: 0.5045\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5022 - val_loss: 0.4984\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4881 - val_loss: 0.4922\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4781 - val_loss: 0.4863\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4671 - val_loss: 0.4829\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4817 - val_loss: 0.4783\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4700 - val_loss: 0.4727\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4524 - val_loss: 0.4685\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4415 - val_loss: 0.4656\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4661 - val_loss: 0.4609\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4469 - val_loss: 0.4577\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4347 - val_loss: 0.4538\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4413 - val_loss: 0.4506\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4359 - val_loss: 0.4472\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4366 - val_loss: 0.4450\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4267 - val_loss: 0.4431\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4273 - val_loss: 0.4388\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4420 - val_loss: 0.4364\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4481 - val_loss: 0.4335\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4268 - val_loss: 0.4318\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4226 - val_loss: 0.4295\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4247 - val_loss: 0.4270\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4199 - val_loss: 0.4249\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.4215 - val_loss: 0.4236\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4251 - val_loss: 0.4209\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3955 - val_loss: 0.4205\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4206 - val_loss: 0.4189\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4195 - val_loss: 0.4158\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3935 - val_loss: 0.4136\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.4057 - val_loss: 0.4129\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4238 - val_loss: 0.4109\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3886 - val_loss: 0.4088\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4198 - val_loss: 0.4081\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3881 - val_loss: 0.4084\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4147 - val_loss: 0.4067\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4184 - val_loss: 0.4042\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4038 - val_loss: 0.4045\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3927 - val_loss: 0.4039\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4061 - val_loss: 0.4019\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3809 - val_loss: 0.4012\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3968 - val_loss: 0.3997\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3963 - val_loss: 0.3983\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3917 - val_loss: 0.4003\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3880 - val_loss: 0.3978\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3885 - val_loss: 0.3956\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3777 - val_loss: 0.3938\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3864 - val_loss: 0.3933\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3845 - val_loss: 0.3939\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3816 - val_loss: 0.3916\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3729 - val_loss: 0.3934\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3986 - val_loss: 0.3911\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4009 - val_loss: 0.3899\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3741 - val_loss: 0.3904\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3937 - val_loss: 0.3888\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3687 - val_loss: 0.3878\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3740 - val_loss: 0.3872\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3731 - val_loss: 0.3885\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3909 - val_loss: 0.3871\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3900 - val_loss: 0.3859\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3727 - val_loss: 0.3859\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3801 - val_loss: 0.3845\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3865 - val_loss: 0.3834\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.3832\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.3639 - val_loss: 0.3856\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.3667 - val_loss: 0.3826\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3833 - val_loss: 0.3838\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3649 - val_loss: 0.3844\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3649 - val_loss: 0.3820\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3723 - val_loss: 0.3812\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3924 - val_loss: 0.3807\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3770 - val_loss: 0.3800\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3790 - val_loss: 0.3782\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3671 - val_loss: 0.3784\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3487 - val_loss: 0.3792\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3845 - val_loss: 0.3805\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3766 - val_loss: 0.3774\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3685 - val_loss: 0.3773\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3745 - val_loss: 0.3764\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3573 - val_loss: 0.3763\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3757 - val_loss: 0.3755\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3694 - val_loss: 0.3742\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3602 - val_loss: 0.3757\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3812 - val_loss: 0.3751\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3737\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3637 - val_loss: 0.3740\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3644 - val_loss: 0.3750\n",
      "121/121 [==============================] - 0s 486us/step - loss: 0.3786\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0724 - val_loss: 1.1271\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 1.0175 - val_loss: 0.7763\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.7595 - val_loss: 0.6889\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.6933 - val_loss: 0.6519\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.6127 - val_loss: 0.6241\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.6196 - val_loss: 0.6035\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.6093 - val_loss: 0.5840\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.5703 - val_loss: 0.5685\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.5184 - val_loss: 0.5556\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.5458 - val_loss: 0.5443\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.5002 - val_loss: 0.5325\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.4984 - val_loss: 0.5223\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.5097 - val_loss: 0.5130\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.4980 - val_loss: 0.5052\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.5022 - val_loss: 0.4978\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4695 - val_loss: 0.4913\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4881 - val_loss: 0.4852\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4601 - val_loss: 0.4803\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.4432 - val_loss: 0.4753\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4765 - val_loss: 0.4691\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4682 - val_loss: 0.4661\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4572 - val_loss: 0.4610\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.4417 - val_loss: 0.4584\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4548\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 0.4508\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4530 - val_loss: 0.4480\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.4265 - val_loss: 0.4468\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4372 - val_loss: 0.4442\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4270 - val_loss: 0.4424\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.4381\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4380 - val_loss: 0.4346\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4335\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.4317\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4162 - val_loss: 0.4311\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4241 - val_loss: 0.4276\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4411 - val_loss: 0.4253\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4232\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.4241\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.4204\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.4193\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.4178\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4335 - val_loss: 0.4171\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.4149\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4161\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4091 - val_loss: 0.4131\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4131\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.4123\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4109\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.4085\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3971 - val_loss: 0.4075\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.4067\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4088 - val_loss: 0.4062\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.4059\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3845 - val_loss: 0.4084\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.4030\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.4023\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3976 - val_loss: 0.4017\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3893 - val_loss: 0.4016\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.3999\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.3992\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3744 - val_loss: 0.4003\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4017 - val_loss: 0.3989\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.3980\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.3960\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.3976\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4010 - val_loss: 0.3967\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.3971\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3896 - val_loss: 0.3952\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.3934\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.3941\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.3941\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.3932\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3627 - val_loss: 0.3920\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3906\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.3915\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.3898\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3810 - val_loss: 0.3899\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.3902\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.3885\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.3873\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3766 - val_loss: 0.3880\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3790 - val_loss: 0.3861\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3923 - val_loss: 0.3885\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3748 - val_loss: 0.3872\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3605 - val_loss: 0.3853\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3770 - val_loss: 0.3844\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3802 - val_loss: 0.3850\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3785 - val_loss: 0.3838\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3871 - val_loss: 0.3836\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.3822\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3713 - val_loss: 0.3828\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.3822\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3831 - val_loss: 0.3823\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3830 - val_loss: 0.3830\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3774 - val_loss: 0.3800\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.3800\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.3796\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3906 - val_loss: 0.3793\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3753 - val_loss: 0.3790\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3785\n",
      "121/121 [==============================] - 0s 498us/step - loss: 0.3786\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.9301 - val_loss: 1.1651\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9013 - val_loss: 0.8105\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7032 - val_loss: 0.7118\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6602 - val_loss: 0.6500\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.6141 - val_loss: 0.6040\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5919 - val_loss: 0.5798\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5622 - val_loss: 0.5644\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.5467 - val_loss: 0.5633\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5369 - val_loss: 0.5695\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5288 - val_loss: 0.5717\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.5215 - val_loss: 0.5716\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.5434 - val_loss: 0.5698\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.5110 - val_loss: 0.5670\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4865 - val_loss: 0.5621\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.5058 - val_loss: 0.5612\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4713 - val_loss: 0.5596\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4891 - val_loss: 0.5554\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4705 - val_loss: 0.5536\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4708 - val_loss: 0.5510\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4440 - val_loss: 0.5478\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4666 - val_loss: 0.5446\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4534 - val_loss: 0.5427\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4576 - val_loss: 0.5399\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4670 - val_loss: 0.5363\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.4504 - val_loss: 0.5333\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4677 - val_loss: 0.5307\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4624 - val_loss: 0.5283\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4382 - val_loss: 0.5253\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.4572 - val_loss: 0.5228\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4546 - val_loss: 0.5185\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4452 - val_loss: 0.5176\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4481 - val_loss: 0.5124\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.4380 - val_loss: 0.5093\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4417 - val_loss: 0.5065\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4257 - val_loss: 0.5048\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.4350 - val_loss: 0.5003\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4188 - val_loss: 0.4978\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4419 - val_loss: 0.4976\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4320 - val_loss: 0.4933\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.4901\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.4891\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.4270 - val_loss: 0.4858\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4075 - val_loss: 0.4820\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3953 - val_loss: 0.4780\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.4022 - val_loss: 0.4771\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4067 - val_loss: 0.4750\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4056 - val_loss: 0.4714\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3962 - val_loss: 0.4693\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3985 - val_loss: 0.4656\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4170 - val_loss: 0.4647\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4012 - val_loss: 0.4631\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4175 - val_loss: 0.4608\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.4575\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4087 - val_loss: 0.4554\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4169 - val_loss: 0.4538\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4081 - val_loss: 0.4527\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3965 - val_loss: 0.4503\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.4025 - val_loss: 0.4465\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4029 - val_loss: 0.4459\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4198 - val_loss: 0.4436\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3920 - val_loss: 0.4431\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4034 - val_loss: 0.4426\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3973 - val_loss: 0.4387\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4194 - val_loss: 0.4368\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.3995 - val_loss: 0.4350\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.3945 - val_loss: 0.4338\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4005 - val_loss: 0.4318\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4009 - val_loss: 0.4305\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3876 - val_loss: 0.4302\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3760 - val_loss: 0.4281\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3927 - val_loss: 0.4261\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3848 - val_loss: 0.4246\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3911 - val_loss: 0.4236\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4195 - val_loss: 0.4218\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4065 - val_loss: 0.4212\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3808 - val_loss: 0.4201\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3999 - val_loss: 0.4186\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3882 - val_loss: 0.4182\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3848 - val_loss: 0.4147\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4028 - val_loss: 0.4150\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3885 - val_loss: 0.4149\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3759 - val_loss: 0.4114\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3834 - val_loss: 0.4115\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3838 - val_loss: 0.4093\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4004 - val_loss: 0.4097\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3988 - val_loss: 0.4108\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3780 - val_loss: 0.4075\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3638 - val_loss: 0.4060\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3835 - val_loss: 0.4057\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3904 - val_loss: 0.4032\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3849 - val_loss: 0.4033\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3773 - val_loss: 0.4042\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3773 - val_loss: 0.4009\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3728 - val_loss: 0.4002\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3869 - val_loss: 0.4020\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.3678 - val_loss: 0.4000\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3945 - val_loss: 0.3998\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3740 - val_loss: 0.4007\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3801 - val_loss: 0.3976\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3718 - val_loss: 0.3965\n",
      "121/121 [==============================] - 0s 485us/step - loss: 0.3865\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5239 - val_loss: 5.2974\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.8152 - val_loss: 60.2230\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 5.8144 - val_loss: 600.2995\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 187.1677 - val_loss: 6298.9219\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 6438.4564 - val_loss: 65805.2266\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 4261.1460 - val_loss: 701494.5000\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 490053.5838 - val_loss: 7290510.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 3602209.0062 - val_loss: 76345688.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 50766400.6687 - val_loss: 801925312.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 1581176555.2181 - val_loss: 8409177600.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 6757672059.6708 - val_loss: 88769298432.0000\n",
      "121/121 [==============================] - 0s 466us/step - loss: 1738399616.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.3187 - val_loss: 2.0429\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 2.1957 - val_loss: 18.3776\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 4.4486 - val_loss: 224.6595\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 62.2639 - val_loss: 2604.6624\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 536.2983 - val_loss: 30874.4375\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 41380.6980 - val_loss: 364033.1562\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 24496.4998 - val_loss: 4343086.0000\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 1630969.9907 - val_loss: 51039808.0000\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 37425626.1218 - val_loss: 601842560.0000\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 191353177.7572 - val_loss: 7124529152.0000\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 28837420423.0453 - val_loss: 84085932032.0000\n",
      "121/121 [==============================] - 0s 518us/step - loss: 2956013824.0000\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1136 - val_loss: 1.2966\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.7445 - val_loss: 0.8257\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.6223 - val_loss: 0.6192\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5625 - val_loss: 0.5537\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.5676 - val_loss: 0.5714\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.5421 - val_loss: 0.6269\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5116 - val_loss: 0.7001\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.5216 - val_loss: 0.7672\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.5323 - val_loss: 0.8392\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4953 - val_loss: 0.9070\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.5269 - val_loss: 0.9628\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5070 - val_loss: 1.0073\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5096 - val_loss: 1.0483\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.5143 - val_loss: 1.0890\n",
      "121/121 [==============================] - 0s 480us/step - loss: 1.2764\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5732 - val_loss: 3.9920\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 2.2049 - val_loss: 2.0415\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 4.7961 - val_loss: 0.6037\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.5007 - val_loss: 0.4307\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4062 - val_loss: 0.3878\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3568 - val_loss: 0.3688\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3570 - val_loss: 0.3708\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3350 - val_loss: 0.3724\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3256 - val_loss: 0.3597\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3372 - val_loss: 0.3768\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3412 - val_loss: 0.3529\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3315 - val_loss: 0.3578\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3336 - val_loss: 0.3516\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3429 - val_loss: 0.3480\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3260 - val_loss: 0.3503\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3058 - val_loss: 0.3514\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3322 - val_loss: 0.3528\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3162 - val_loss: 0.3503\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3263 - val_loss: 0.3631\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3299 - val_loss: 0.3383\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3308 - val_loss: 0.3478\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3160 - val_loss: 0.3388\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3260 - val_loss: 0.3362\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3112 - val_loss: 0.3351\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3299 - val_loss: 0.3372\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3162 - val_loss: 0.3317\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.2982 - val_loss: 0.3444\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3116 - val_loss: 0.3302\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3000 - val_loss: 0.3582\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3009 - val_loss: 0.3272\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3098 - val_loss: 0.3249\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3047 - val_loss: 0.3285\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3229 - val_loss: 0.3475\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3112 - val_loss: 0.3299\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3031 - val_loss: 0.3297\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3096 - val_loss: 0.3281\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.2913 - val_loss: 0.3215\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.2992 - val_loss: 0.3249\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.2970 - val_loss: 0.3204\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2992 - val_loss: 0.3201\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3029 - val_loss: 0.3205\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.2924 - val_loss: 0.3160\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3056 - val_loss: 0.3230\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3081 - val_loss: 0.3241\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3137 - val_loss: 0.3228\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3032 - val_loss: 0.3171\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2912 - val_loss: 0.3172\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.2846 - val_loss: 0.3172\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3046 - val_loss: 0.3173\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3010 - val_loss: 0.3158\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.2965 - val_loss: 0.3177\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.2831 - val_loss: 0.3131\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.2976 - val_loss: 0.3158\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3133 - val_loss: 0.3202\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.2982 - val_loss: 0.3191\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3124 - val_loss: 0.3147\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3015 - val_loss: 0.3125\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.2852 - val_loss: 0.3134\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.2954 - val_loss: 0.3105\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.2967 - val_loss: 0.3096\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3066 - val_loss: 0.3121\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.2972 - val_loss: 0.3131\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3038 - val_loss: 0.3071\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.2832 - val_loss: 0.3100\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.2940 - val_loss: 0.3093\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.2962 - val_loss: 0.3134\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.2817 - val_loss: 0.3091\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3086 - val_loss: 0.3093\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3081 - val_loss: 0.3073\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.2956 - val_loss: 0.3153\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.2990 - val_loss: 0.3183\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3007 - val_loss: 0.3064\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.2843 - val_loss: 0.3052\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.2950 - val_loss: 0.3108\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3034 - val_loss: 0.3086\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.2755 - val_loss: 0.3129\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.2842 - val_loss: 0.3073\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.2854 - val_loss: 0.3042\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.2919 - val_loss: 0.3124\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.2896 - val_loss: 0.3127\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.2875 - val_loss: 0.3091\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.2949 - val_loss: 0.3019\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.2863 - val_loss: 0.3073\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2899 - val_loss: 0.3050\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3033 - val_loss: 0.3042\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2965 - val_loss: 0.3089\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2934 - val_loss: 0.3067\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2800 - val_loss: 0.3069\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.2960 - val_loss: 0.3022\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.2831 - val_loss: 0.3088\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.2954 - val_loss: 0.3012\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2759 - val_loss: 0.2997\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2926 - val_loss: 0.3005\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.2885 - val_loss: 0.3024\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.2941 - val_loss: 0.3051\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.2806 - val_loss: 0.3049\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.2858 - val_loss: 0.3058\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.2937 - val_loss: 0.3056\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.2756 - val_loss: 0.3013\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.2850 - val_loss: 0.3056\n",
      "121/121 [==============================] - 0s 484us/step - loss: 0.3262\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.7437 - val_loss: 2.7709\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 7.5653 - val_loss: 0.5737\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6511 - val_loss: 0.5112\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4459 - val_loss: 0.4317\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3917 - val_loss: 0.4210\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3965 - val_loss: 0.4008\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3838 - val_loss: 0.3960\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3762 - val_loss: 0.3977\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3996 - val_loss: 0.3917\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3618 - val_loss: 0.3851\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3805 - val_loss: 0.3841\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3556 - val_loss: 0.3900\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3801 - val_loss: 0.3783\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3706 - val_loss: 0.3796\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3798\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3516 - val_loss: 0.3752\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3687 - val_loss: 0.3685\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3595 - val_loss: 0.3698\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3703\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3637 - val_loss: 0.3663\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3503 - val_loss: 0.3771\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3552 - val_loss: 0.3697\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3616 - val_loss: 0.3672\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3512 - val_loss: 0.3634\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3656 - val_loss: 0.3629\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3535 - val_loss: 0.3639\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3429 - val_loss: 0.3600\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3501 - val_loss: 0.3715\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3668 - val_loss: 0.3589\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3497 - val_loss: 0.3733\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3508 - val_loss: 0.3691\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3604 - val_loss: 0.3638\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3537 - val_loss: 0.3605\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3415 - val_loss: 0.3630\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3371 - val_loss: 0.3588\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3441 - val_loss: 0.3615\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3534 - val_loss: 0.3591\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3453 - val_loss: 0.3608\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.3488 - val_loss: 0.3555\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3384 - val_loss: 0.4203\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3706 - val_loss: 0.3540\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3250 - val_loss: 0.3580\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3240 - val_loss: 0.3550\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3469 - val_loss: 0.3529\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3402 - val_loss: 0.3543\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.3330 - val_loss: 0.3631\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3257 - val_loss: 0.3541\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3429 - val_loss: 0.3523\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3405 - val_loss: 0.3515\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3274 - val_loss: 0.3522\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3216 - val_loss: 0.3477\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3230 - val_loss: 0.3528\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3256 - val_loss: 0.3563\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3383 - val_loss: 0.3509\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3305 - val_loss: 0.3497\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3453 - val_loss: 0.3504\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3343 - val_loss: 0.3475\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3280 - val_loss: 0.3555\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3345 - val_loss: 0.3486\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3305 - val_loss: 0.3533\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3213 - val_loss: 0.3479\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3295 - val_loss: 0.3563\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3543 - val_loss: 0.3477\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3391 - val_loss: 0.3483\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3305 - val_loss: 0.3691\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3211 - val_loss: 0.3535\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3370 - val_loss: 0.3503\n",
      "121/121 [==============================] - 0s 493us/step - loss: 0.3431\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5329 - val_loss: 0.7732\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.6099 - val_loss: 0.5702\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.5309 - val_loss: 0.5074\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.4885 - val_loss: 0.4927\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4640 - val_loss: 0.4840\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4512 - val_loss: 0.4881\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4286 - val_loss: 0.4750\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4211 - val_loss: 0.4599\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.4336 - val_loss: 0.4533\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3963 - val_loss: 0.4496\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4160 - val_loss: 0.4404\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3969 - val_loss: 0.4307\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.4059 - val_loss: 0.4226\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3941 - val_loss: 0.4191\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3968 - val_loss: 0.4082\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3971 - val_loss: 0.4025\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3942 - val_loss: 0.3963\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3672 - val_loss: 0.4042\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.3943 - val_loss: 0.3885\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3795 - val_loss: 0.3883\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3871 - val_loss: 0.3849\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3799 - val_loss: 0.3793\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3854 - val_loss: 0.3814\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.3633 - val_loss: 0.3773\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3441 - val_loss: 0.3769\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3701 - val_loss: 0.3771\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3395 - val_loss: 0.3733\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3601 - val_loss: 0.3710\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3580 - val_loss: 0.3722\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3519 - val_loss: 0.3744\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3556 - val_loss: 0.3686\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3678 - val_loss: 0.3723\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3469 - val_loss: 0.3703\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3561 - val_loss: 0.3728\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3568 - val_loss: 0.3639\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3324 - val_loss: 0.3664\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.3638\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3717 - val_loss: 0.3648\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3655 - val_loss: 0.3647\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3342 - val_loss: 0.3691\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3429 - val_loss: 0.3618\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3378 - val_loss: 0.3712\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.3498 - val_loss: 0.3668\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.3644\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3542 - val_loss: 0.3825\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3589 - val_loss: 0.3603\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3314 - val_loss: 0.3649\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.3348 - val_loss: 0.3693\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.3469 - val_loss: 0.3610\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3682 - val_loss: 0.3594\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3277 - val_loss: 0.3627\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.3661\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3321 - val_loss: 0.3837\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3597\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.3622\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3613\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.3588\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3559\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3640\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3472 - val_loss: 0.3591\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3430 - val_loss: 0.3598\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3590\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3233 - val_loss: 0.3675\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3562\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3582\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3704\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.3538\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3581\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3327 - val_loss: 0.3590\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3231 - val_loss: 0.3523\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3144 - val_loss: 0.3528\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3521\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3545\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3311 - val_loss: 0.3505\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3497\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3506\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3938\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3507\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3508\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.3600\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3445 - val_loss: 0.3521\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3205 - val_loss: 0.3459\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3275 - val_loss: 0.3461\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3193 - val_loss: 0.3473\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.3498\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3345 - val_loss: 0.3445\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3195 - val_loss: 0.3453\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3447\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3141 - val_loss: 0.3474\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3157 - val_loss: 0.3406\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3232 - val_loss: 0.3460\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3414\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3301 - val_loss: 0.3463\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3255 - val_loss: 0.3411\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.4305\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3405 - val_loss: 0.3406\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3210 - val_loss: 0.3398\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3388\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3048 - val_loss: 0.3393\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3275 - val_loss: 0.3467\n",
      "121/121 [==============================] - 0s 615us/step - loss: 0.3577\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.2463 - val_loss: 0.9358\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8599 - val_loss: 0.6775\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6375 - val_loss: 0.6299\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5919 - val_loss: 0.5976\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5536 - val_loss: 0.5740\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5383 - val_loss: 0.5481\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5121 - val_loss: 0.5292\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4904 - val_loss: 0.5127\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4722 - val_loss: 0.4985\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4677 - val_loss: 0.4842\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4763 - val_loss: 0.4729\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4649\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4414 - val_loss: 0.4537\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.4497\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.4329 - val_loss: 0.4394\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4278 - val_loss: 0.4332\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.4256\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4177 - val_loss: 0.4201\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4207 - val_loss: 0.4149\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4057 - val_loss: 0.4098\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4153 - val_loss: 0.4052\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3811 - val_loss: 0.4019\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3895 - val_loss: 0.3990\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3659 - val_loss: 0.3956\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3769 - val_loss: 0.3917\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3804 - val_loss: 0.3913\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3598 - val_loss: 0.3864\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3725 - val_loss: 0.3841\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3915 - val_loss: 0.3814\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3647 - val_loss: 0.3792\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3792 - val_loss: 0.3787\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3650 - val_loss: 0.3777\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3586 - val_loss: 0.3739\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.3747\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.3717\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3786 - val_loss: 0.3686\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3632 - val_loss: 0.3682\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.3670\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3652\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3632 - val_loss: 0.3632\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3587 - val_loss: 0.3638\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3539 - val_loss: 0.3627\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3499 - val_loss: 0.3595\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.3625\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.3581\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3558\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3543\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3374 - val_loss: 0.3542\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3628 - val_loss: 0.3533\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3530\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3503 - val_loss: 0.3523\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3506\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3270 - val_loss: 0.3494\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3384 - val_loss: 0.3488\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3417 - val_loss: 0.3520\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3473\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3275 - val_loss: 0.3486\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.3469\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 0.3453\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3467\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3208 - val_loss: 0.3444\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 0.3437\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3297 - val_loss: 0.3426\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3003 - val_loss: 0.3412\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3419\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.3415\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3390\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.3405\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3222 - val_loss: 0.3386\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3382\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3267 - val_loss: 0.3395\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.3386\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3040 - val_loss: 0.3377\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.3396\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3356 - val_loss: 0.3361\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3162 - val_loss: 0.3342\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3337\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2997 - val_loss: 0.3335\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3257 - val_loss: 0.3331\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3138 - val_loss: 0.3324\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3189 - val_loss: 0.3342\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3315\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3310\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3156 - val_loss: 0.3327\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3107 - val_loss: 0.3296\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3082 - val_loss: 0.3304\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3061 - val_loss: 0.3340\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.3301\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.3324\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 0.3289\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2952 - val_loss: 0.3272\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.3291\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3049 - val_loss: 0.3270\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3035 - val_loss: 0.3315\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2946 - val_loss: 0.3278\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.2985 - val_loss: 0.3237\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3028 - val_loss: 0.3297\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3145 - val_loss: 0.3252\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.2884 - val_loss: 0.3253\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3029 - val_loss: 0.3242\n",
      "121/121 [==============================] - 0s 501us/step - loss: 0.3425\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.8447 - val_loss: 1.1857\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8274 - val_loss: 0.6527\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6417 - val_loss: 0.6043\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5681 - val_loss: 0.5731\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5437 - val_loss: 0.5490\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5261 - val_loss: 0.5284\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4826 - val_loss: 0.5129\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5013 - val_loss: 0.4953\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4721 - val_loss: 0.4839\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4433 - val_loss: 0.4731\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4556 - val_loss: 0.4628\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4206 - val_loss: 0.4542\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4456\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.4399\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4321\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.4305\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.4208\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.4156\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3886 - val_loss: 0.4104\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.4080\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3828 - val_loss: 0.4038\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3915 - val_loss: 0.4009\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3928 - val_loss: 0.3970\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3850 - val_loss: 0.3962\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.3918\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3680 - val_loss: 0.3900\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3824 - val_loss: 0.3866\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3795 - val_loss: 0.3853\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3644 - val_loss: 0.3845\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3803\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3706 - val_loss: 0.3787\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3783 - val_loss: 0.3773\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3582 - val_loss: 0.3756\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3765\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3723\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3702\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3694\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3560 - val_loss: 0.3677\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3426 - val_loss: 0.3675\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3655\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3646\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3505 - val_loss: 0.3647\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3537 - val_loss: 0.3628\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3622\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3391 - val_loss: 0.3618\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3621\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3292 - val_loss: 0.3574\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3583\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3346 - val_loss: 0.3556\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.3592\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3508 - val_loss: 0.3576\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3438 - val_loss: 0.3542\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3428 - val_loss: 0.3537\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3481 - val_loss: 0.3570\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3497 - val_loss: 0.3519\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3496 - val_loss: 0.3515\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3312 - val_loss: 0.3505\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3495 - val_loss: 0.3501\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3485\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3393 - val_loss: 0.3557\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3277 - val_loss: 0.3509\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3385 - val_loss: 0.3477\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3430 - val_loss: 0.3467\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3280 - val_loss: 0.3444\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3363 - val_loss: 0.3459\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3219 - val_loss: 0.3448\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3323 - val_loss: 0.3435\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3271 - val_loss: 0.3443\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3306 - val_loss: 0.3430\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3132 - val_loss: 0.3412\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3249 - val_loss: 0.3440\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3278 - val_loss: 0.3408\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3424\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3245 - val_loss: 0.3388\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3226 - val_loss: 0.3487\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3274 - val_loss: 0.3377\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3301 - val_loss: 0.3455\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3247 - val_loss: 0.3395\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3410\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3119 - val_loss: 0.3379\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3290 - val_loss: 0.3411\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3237 - val_loss: 0.3355\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3066 - val_loss: 0.3410\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3165 - val_loss: 0.3332\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3025 - val_loss: 0.3379\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3113 - val_loss: 0.3327\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3253 - val_loss: 0.3385\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3145 - val_loss: 0.3331\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3098 - val_loss: 0.3392\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3166 - val_loss: 0.3301\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3172 - val_loss: 0.3353\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3094 - val_loss: 0.3297\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3187 - val_loss: 0.3343\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3100 - val_loss: 0.3284\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3205 - val_loss: 0.3337\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3291\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3054 - val_loss: 0.3281\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.2911 - val_loss: 0.3326\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3066 - val_loss: 0.3290\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.2999 - val_loss: 0.3328\n",
      "121/121 [==============================] - 0s 498us/step - loss: 0.3298\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.1262 - val_loss: 1.0388\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7801 - val_loss: 0.8787\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6929 - val_loss: 0.7737\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6966\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6017 - val_loss: 0.6450\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5646 - val_loss: 0.5902\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5159 - val_loss: 0.5564\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5196 - val_loss: 0.5292\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5186 - val_loss: 0.5088\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4905 - val_loss: 0.4939\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4813 - val_loss: 0.4834\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4703 - val_loss: 0.4799\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.4399 - val_loss: 0.4716\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4391 - val_loss: 0.4680\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4315 - val_loss: 0.4629\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4421 - val_loss: 0.4602\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4224 - val_loss: 0.4576\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4171 - val_loss: 0.4529\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.4522\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.4483\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4004 - val_loss: 0.4429\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4073 - val_loss: 0.4400\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3906 - val_loss: 0.4357\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4131 - val_loss: 0.4323\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3850 - val_loss: 0.4294\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4025 - val_loss: 0.4257\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3935 - val_loss: 0.4207\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3987 - val_loss: 0.4175\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3795 - val_loss: 0.4164\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3904 - val_loss: 0.4138\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4113\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3801 - val_loss: 0.4074\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3759 - val_loss: 0.4047\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3760 - val_loss: 0.4016\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3601 - val_loss: 0.3985\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3595 - val_loss: 0.3988\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3637 - val_loss: 0.3940\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3591 - val_loss: 0.3933\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3658 - val_loss: 0.3961\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3535 - val_loss: 0.3930\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3638 - val_loss: 0.3859\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3589 - val_loss: 0.3883\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3508 - val_loss: 0.3826\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3481 - val_loss: 0.3828\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3646 - val_loss: 0.3781\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3614 - val_loss: 0.3776\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3479 - val_loss: 0.3777\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.3614 - val_loss: 0.3745\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3473 - val_loss: 0.3752\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3704\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3562 - val_loss: 0.3690\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3403 - val_loss: 0.3699\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3663\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3645\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3379 - val_loss: 0.3652\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3614\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3609\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3612\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3580\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3573\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3576\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3429 - val_loss: 0.3562\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3113 - val_loss: 0.3533\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3537\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3398 - val_loss: 0.3522\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3551 - val_loss: 0.3512\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3391 - val_loss: 0.3514\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3337 - val_loss: 0.3494\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3257 - val_loss: 0.3470\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3321 - val_loss: 0.3482\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3389 - val_loss: 0.3451\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.3518\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3156 - val_loss: 0.3465\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3236 - val_loss: 0.3446\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3457\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3110 - val_loss: 0.3426\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3445\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3426\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3291 - val_loss: 0.3417\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.3411\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3376\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3269 - val_loss: 0.3381\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3245 - val_loss: 0.3389\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3299 - val_loss: 0.3392\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3081 - val_loss: 0.3363\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3259 - val_loss: 0.3375\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3286 - val_loss: 0.3364\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3155 - val_loss: 0.3354\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3061 - val_loss: 0.3352\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3293 - val_loss: 0.3328\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3198 - val_loss: 0.3371\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3312\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3346 - val_loss: 0.3313\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2990 - val_loss: 0.3287\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3189 - val_loss: 0.3286\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.3285\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3224 - val_loss: 0.3297\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3117 - val_loss: 0.3276\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3189 - val_loss: 0.3260\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3036 - val_loss: 0.3268\n",
      "121/121 [==============================] - 0s 536us/step - loss: 0.3273\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 1.2353 - val_loss: 0.4757\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4802 - val_loss: 0.8151\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4772 - val_loss: 2.6221\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 85.1304 - val_loss: 0.9092\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.8961 - val_loss: 0.8675\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 1.0025 - val_loss: 0.9062\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.9219 - val_loss: 0.8496\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8516 - val_loss: 0.7978\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.8263 - val_loss: 0.7284\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.8782 - val_loss: 0.7226\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.7397 - val_loss: 0.7707\n",
      "121/121 [==============================] - 0s 506us/step - loss: 0.7802\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4835 - val_loss: 1.1004\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.7014 - val_loss: 0.5318\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5773 - val_loss: 1.6074\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 2.7341 - val_loss: 1.5051\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4591 - val_loss: 0.4242\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 504us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1694 - val_loss: 0.5058\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4759 - val_loss: 0.4443\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4170 - val_loss: 0.4124\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4033 - val_loss: 0.4159\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3824 - val_loss: 0.3820\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3650 - val_loss: 0.3824\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3640 - val_loss: 0.3825\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3577 - val_loss: 0.3604\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3606 - val_loss: 0.3718\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3433 - val_loss: 0.3575\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3636 - val_loss: 0.3428\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3423 - val_loss: 0.3540\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3564 - val_loss: 0.3387\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3418 - val_loss: 0.3745\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3159 - val_loss: 0.3565\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3407 - val_loss: 0.3370\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3239 - val_loss: 0.3577\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3235 - val_loss: 0.3377\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.3409\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3155 - val_loss: 0.3237\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3156 - val_loss: 0.3429\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3195 - val_loss: 0.3356\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3130 - val_loss: 0.3303\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3202 - val_loss: 0.3641\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3258 - val_loss: 0.3254\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.2993 - val_loss: 0.3244\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3348\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3107 - val_loss: 0.3255\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3396\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.2937 - val_loss: 0.3318\n",
      "121/121 [==============================] - 0s 983us/step - loss: 0.3309\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 5.6560 - val_loss: 3.0413\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.6040 - val_loss: 1.6952\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.5271 - val_loss: 1.1600\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1241 - val_loss: 0.9267\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9077 - val_loss: 0.8176\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8345 - val_loss: 0.7640\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7688 - val_loss: 0.7340\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7395 - val_loss: 0.7153\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6817 - val_loss: 0.7017\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.7138 - val_loss: 0.6907\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.6631 - val_loss: 0.6809\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.6948 - val_loss: 0.6717\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.6652 - val_loss: 0.6631\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.6638 - val_loss: 0.6549\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.6474 - val_loss: 0.6472\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.6110 - val_loss: 0.6397\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.6246 - val_loss: 0.6327\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.6181 - val_loss: 0.6260\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.6321 - val_loss: 0.6195\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6277 - val_loss: 0.6135\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6114 - val_loss: 0.6077\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5768 - val_loss: 0.6022\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5784 - val_loss: 0.5969\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5884 - val_loss: 0.5917\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.5750 - val_loss: 0.5869\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.5540 - val_loss: 0.5822\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.5648 - val_loss: 0.5777\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.5701 - val_loss: 0.5735\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5445 - val_loss: 0.5695\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5547 - val_loss: 0.5656\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5436 - val_loss: 0.5618\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5448 - val_loss: 0.5581\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5450 - val_loss: 0.5546\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5120 - val_loss: 0.5512\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.5450 - val_loss: 0.5481\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5106 - val_loss: 0.5450\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5322 - val_loss: 0.5420\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5173 - val_loss: 0.5391\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5460 - val_loss: 0.5363\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5303 - val_loss: 0.5336\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.5285 - val_loss: 0.5311\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5361 - val_loss: 0.5284\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.4944 - val_loss: 0.5258\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.5212 - val_loss: 0.5234\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.5126 - val_loss: 0.5209\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.5140 - val_loss: 0.5185\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5144 - val_loss: 0.5162\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.4884 - val_loss: 0.5142\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4934 - val_loss: 0.5120\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.4791 - val_loss: 0.5100\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4946 - val_loss: 0.5079\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.4701 - val_loss: 0.5060\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4798 - val_loss: 0.5042\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4932 - val_loss: 0.5024\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4929 - val_loss: 0.5005\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4513 - val_loss: 0.4989\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.4869 - val_loss: 0.4973\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4583 - val_loss: 0.4957\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4703 - val_loss: 0.4941\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4772 - val_loss: 0.4926\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4569 - val_loss: 0.4912\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4807 - val_loss: 0.4898\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4878 - val_loss: 0.4884\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4820 - val_loss: 0.4871\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4783 - val_loss: 0.4858\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4581 - val_loss: 0.4848\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.4729 - val_loss: 0.4836\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4600 - val_loss: 0.4823\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.4839 - val_loss: 0.4812\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4464 - val_loss: 0.4800\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4773 - val_loss: 0.4788\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4734 - val_loss: 0.4778\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4609 - val_loss: 0.4768\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.4629 - val_loss: 0.4757\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4649 - val_loss: 0.4748\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4736 - val_loss: 0.4738\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4520 - val_loss: 0.4729\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4526 - val_loss: 0.4720\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4711\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4477 - val_loss: 0.4701\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4530 - val_loss: 0.4692\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4632 - val_loss: 0.4684\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4730 - val_loss: 0.4676\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.4487 - val_loss: 0.4669\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4661\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4369 - val_loss: 0.4653\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4623 - val_loss: 0.4644\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.4638\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4489 - val_loss: 0.4630\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.4422 - val_loss: 0.4624\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4478 - val_loss: 0.4616\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4391 - val_loss: 0.4608\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4441 - val_loss: 0.4602\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4540 - val_loss: 0.4595\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4403 - val_loss: 0.4587\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4676 - val_loss: 0.4581\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4313 - val_loss: 0.4574\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4187 - val_loss: 0.4566\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4208 - val_loss: 0.4560\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4354 - val_loss: 0.4553\n",
      "121/121 [==============================] - 0s 604us/step - loss: 0.4495\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.8214 - val_loss: 2.7957\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 2.4469 - val_loss: 1.6919\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 1.5467 - val_loss: 1.2051\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 1.1392 - val_loss: 0.9663\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.9282 - val_loss: 0.8471\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.8305 - val_loss: 0.7843\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.7642 - val_loss: 0.7486\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.7106 - val_loss: 0.7268\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6695 - val_loss: 0.7118\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.6639 - val_loss: 0.6999\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.6750 - val_loss: 0.6900\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.6674 - val_loss: 0.6816\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.6474 - val_loss: 0.6739\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.6563 - val_loss: 0.6664\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.6688 - val_loss: 0.6597\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.6673 - val_loss: 0.6529\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6178 - val_loss: 0.6467\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.6270 - val_loss: 0.6409\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.6496 - val_loss: 0.6351\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6786 - val_loss: 0.6297\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.6052 - val_loss: 0.6244\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.5870 - val_loss: 0.6191\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.6258 - val_loss: 0.6143\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5892 - val_loss: 0.6095\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5845 - val_loss: 0.6048\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5823 - val_loss: 0.6005\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5588 - val_loss: 0.5961\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5807 - val_loss: 0.5918\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5598 - val_loss: 0.5880\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.5747 - val_loss: 0.5839\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5650 - val_loss: 0.5803\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5602 - val_loss: 0.5766\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5371 - val_loss: 0.5730\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5686 - val_loss: 0.5695\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5610 - val_loss: 0.5661\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.5389 - val_loss: 0.5627\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.5557 - val_loss: 0.5595\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.5638 - val_loss: 0.5564\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.5453 - val_loss: 0.5532\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5798 - val_loss: 0.5503\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5351 - val_loss: 0.5478\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5133 - val_loss: 0.5450\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.5222 - val_loss: 0.5425\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.5438 - val_loss: 0.5399\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4845 - val_loss: 0.5375\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.5297 - val_loss: 0.5351\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5056 - val_loss: 0.5327\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5063 - val_loss: 0.5306\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.5050 - val_loss: 0.5284\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4846 - val_loss: 0.5264\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.5272 - val_loss: 0.5243\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.5243 - val_loss: 0.5225\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5000 - val_loss: 0.5203\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5042 - val_loss: 0.5186\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5070 - val_loss: 0.5168\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4964 - val_loss: 0.5150\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4953 - val_loss: 0.5133\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4891 - val_loss: 0.5117\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4740 - val_loss: 0.5101\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4778 - val_loss: 0.5085\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4664 - val_loss: 0.5072\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4881 - val_loss: 0.5056\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4901 - val_loss: 0.5042\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4808 - val_loss: 0.5028\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4845 - val_loss: 0.5015\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4867 - val_loss: 0.5001\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4746 - val_loss: 0.4988\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4737 - val_loss: 0.4976\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4542 - val_loss: 0.4963\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.4783 - val_loss: 0.4951\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4728 - val_loss: 0.4940\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4675 - val_loss: 0.4929\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4731 - val_loss: 0.4917\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4525 - val_loss: 0.4906\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4646 - val_loss: 0.4896\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4546 - val_loss: 0.4886\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4772 - val_loss: 0.4875\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4755 - val_loss: 0.4865\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.4599 - val_loss: 0.4855\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4518 - val_loss: 0.4845\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4868 - val_loss: 0.4837\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.4667 - val_loss: 0.4827\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4613 - val_loss: 0.4817\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.4666 - val_loss: 0.4809\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4620 - val_loss: 0.4801\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4538 - val_loss: 0.4792\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4676 - val_loss: 0.4784\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4650 - val_loss: 0.4776\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4609 - val_loss: 0.4769\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4510 - val_loss: 0.4760\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.4594 - val_loss: 0.4751\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4597 - val_loss: 0.4744\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4636 - val_loss: 0.4735\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4547 - val_loss: 0.4727\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.4460 - val_loss: 0.4720\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4565 - val_loss: 0.4712\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.4584 - val_loss: 0.4706\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4416 - val_loss: 0.4699\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.4537 - val_loss: 0.4692\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4344 - val_loss: 0.4685\n",
      "121/121 [==============================] - 0s 545us/step - loss: 0.4518\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.6323 - val_loss: 2.8518\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 2.6483 - val_loss: 1.9062\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 1.7387 - val_loss: 1.5178\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 1.2889 - val_loss: 1.3340\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1304 - val_loss: 1.2269\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.9957 - val_loss: 1.1499\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.8566 - val_loss: 1.0912\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.8486 - val_loss: 1.0423\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.8061 - val_loss: 0.9987\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.7370 - val_loss: 0.9604\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.7362 - val_loss: 0.9271\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.7325 - val_loss: 0.8968\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.7085 - val_loss: 0.8694\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.7061 - val_loss: 0.8439\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.6934 - val_loss: 0.8198\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.6747 - val_loss: 0.7986\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6953 - val_loss: 0.7783\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6516 - val_loss: 0.7591\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6455 - val_loss: 0.7416\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.6570 - val_loss: 0.7251\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.6490 - val_loss: 0.7092\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.6285 - val_loss: 0.6954\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.6245 - val_loss: 0.6816\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.6363 - val_loss: 0.6680\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.5929 - val_loss: 0.6562\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.6169 - val_loss: 0.6453\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.6089 - val_loss: 0.6348\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.6054 - val_loss: 0.6247\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.6052 - val_loss: 0.6159\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.5849 - val_loss: 0.6077\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5548 - val_loss: 0.5995\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5713 - val_loss: 0.5921\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5579 - val_loss: 0.5852\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5653 - val_loss: 0.5791\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.5585 - val_loss: 0.5729\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5656 - val_loss: 0.5677\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5775 - val_loss: 0.5626\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.5513 - val_loss: 0.5581\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.5294 - val_loss: 0.5538\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5514 - val_loss: 0.5499\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5254 - val_loss: 0.5465\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5184 - val_loss: 0.5432\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.5336 - val_loss: 0.5401\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.5354 - val_loss: 0.5373\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.5330 - val_loss: 0.5349\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.5316 - val_loss: 0.5328\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.5073 - val_loss: 0.5308\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5019 - val_loss: 0.5292\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.5124 - val_loss: 0.5277\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5300 - val_loss: 0.5264\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.4948 - val_loss: 0.5253\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5232 - val_loss: 0.5243\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5357 - val_loss: 0.5236\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5064 - val_loss: 0.5229\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.5002 - val_loss: 0.5225\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4997 - val_loss: 0.5222\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4740 - val_loss: 0.5219\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4925 - val_loss: 0.5217\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.5074 - val_loss: 0.5216\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.4966 - val_loss: 0.5219\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.4645 - val_loss: 0.5220\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.5067 - val_loss: 0.5224\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4664 - val_loss: 0.5228\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4931 - val_loss: 0.5233\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4692 - val_loss: 0.5238\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.4940 - val_loss: 0.5243\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4685 - val_loss: 0.5250\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4610 - val_loss: 0.5259\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4589 - val_loss: 0.5266\n",
      "121/121 [==============================] - 0s 495us/step - loss: 0.4996\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.5764 - val_loss: 1.7401\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.8589 - val_loss: 2.2839\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 1.0686 - val_loss: 3.9327\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.9240 - val_loss: 6.6589\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2401 - val_loss: 12.5757\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 15.2262 - val_loss: 24.8011\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 11.8878 - val_loss: 47.9622\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 15.7843 - val_loss: 92.3002\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 120.2023 - val_loss: 177.2294\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 97.9909 - val_loss: 346.3994\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 355.2945 - val_loss: 670.5099\n",
      "121/121 [==============================] - 0s 501us/step - loss: 14.9643\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1811 - val_loss: 2.6999\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 1.1194 - val_loss: 4.3848\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.6653 - val_loss: 8.7565\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 10.3629 - val_loss: 18.2665\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 1.3945 - val_loss: 38.2798\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 17.5621 - val_loss: 78.5802\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 13.0042 - val_loss: 163.4580\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 236.5959 - val_loss: 341.6591\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 357.9186 - val_loss: 702.5179\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 28.0391 - val_loss: 1460.4387\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 903.5764 - val_loss: 2997.4351\n",
      "121/121 [==============================] - 0s 529us/step - loss: 109.5142\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1751 - val_loss: 1.5051\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.7543 - val_loss: 1.0999\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.6271 - val_loss: 0.8804\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5684 - val_loss: 0.7309\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5783 - val_loss: 0.6337\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5660 - val_loss: 0.5741\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.5360 - val_loss: 0.5465\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5393 - val_loss: 0.5397\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.5270 - val_loss: 0.5481\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5488 - val_loss: 0.5692\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5211 - val_loss: 0.5984\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.6328\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5403 - val_loss: 0.6674\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.5448 - val_loss: 0.7056\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5251 - val_loss: 0.7449\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.5185 - val_loss: 0.7840\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.5140 - val_loss: 0.8226\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5162 - val_loss: 0.8566\n",
      "121/121 [==============================] - 0s 508us/step - loss: 0.9548\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3413 - val_loss: 0.5146\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5063 - val_loss: 0.4854\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4711 - val_loss: 0.4523\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4420 - val_loss: 0.4540\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4355 - val_loss: 0.4557\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4316\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.4216 - val_loss: 0.4353\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.4186 - val_loss: 0.4316\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4198 - val_loss: 0.4253\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4312 - val_loss: 0.4246\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.4332 - val_loss: 0.4112\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4077 - val_loss: 0.4122\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4071 - val_loss: 0.4042\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.4130 - val_loss: 0.4163\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4048 - val_loss: 0.4005\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.4073 - val_loss: 0.3983\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3930 - val_loss: 0.3941\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3897 - val_loss: 0.3939\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4004 - val_loss: 0.4231\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3986 - val_loss: 0.3938\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3996 - val_loss: 0.3894\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3760 - val_loss: 0.3952\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.4047 - val_loss: 0.3896\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3861 - val_loss: 0.3863\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3793 - val_loss: 0.3979\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3700 - val_loss: 0.3815\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.3817\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3824 - val_loss: 0.4298\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4025 - val_loss: 0.3906\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3677 - val_loss: 0.3811\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3801 - val_loss: 0.3883\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3673 - val_loss: 0.3783\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3730 - val_loss: 0.3768\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3692 - val_loss: 0.3776\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3648 - val_loss: 0.3766\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3813 - val_loss: 0.3775\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.3809 - val_loss: 0.3813\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.3646 - val_loss: 0.3856\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3756 - val_loss: 0.3741\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3812 - val_loss: 0.3746\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3815 - val_loss: 0.3854\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3603 - val_loss: 0.3774\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3802 - val_loss: 0.3783\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3705 - val_loss: 0.3753\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3621 - val_loss: 0.3852\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3816 - val_loss: 0.4373\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3673 - val_loss: 0.4247\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3714 - val_loss: 0.3780\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3633 - val_loss: 0.3804\n",
      "121/121 [==============================] - 0s 542us/step - loss: 0.3837\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3438 - val_loss: 5.7603\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 553us/step - loss: nan\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3267 - val_loss: 0.5502\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.5086 - val_loss: 0.5119\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.4730 - val_loss: 0.4860\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4414 - val_loss: 0.4717\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4297 - val_loss: 0.4490\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.4233 - val_loss: 0.4443\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4119 - val_loss: 0.4389\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4130 - val_loss: 0.4213\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3946 - val_loss: 0.4201\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4132 - val_loss: 0.4124\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4016 - val_loss: 0.4096\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3878 - val_loss: 0.4085\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3924 - val_loss: 0.4107\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3922 - val_loss: 0.3923\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3959 - val_loss: 0.3944\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4021 - val_loss: 0.3928\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3977 - val_loss: 0.4270\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3949 - val_loss: 0.4002\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3945 - val_loss: 0.3877\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3745 - val_loss: 0.3851\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3721 - val_loss: 0.3852\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.3752 - val_loss: 0.3952\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3765 - val_loss: 0.3836\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3818 - val_loss: 0.4018\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3760 - val_loss: 0.3808\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.3812\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.3645 - val_loss: 0.3828\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.3695 - val_loss: 0.3805\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.3838\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3886 - val_loss: 0.3802\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3836 - val_loss: 0.3844\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3615 - val_loss: 0.4274\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3883 - val_loss: 0.3948\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3694 - val_loss: 0.3795\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3702 - val_loss: 0.3811\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3887 - val_loss: 0.3774\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3739 - val_loss: 0.3885\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.3640 - val_loss: 0.3901\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3585 - val_loss: 0.3800\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3668 - val_loss: 0.3824\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3768 - val_loss: 0.3853\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3752 - val_loss: 0.3796\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3731 - val_loss: 0.3764\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3842 - val_loss: 0.3890\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3653 - val_loss: 0.3770\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.3730 - val_loss: 0.3765\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.3772\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3632 - val_loss: 0.3944\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3794 - val_loss: 0.3780\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.3638 - val_loss: 0.4060\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3634 - val_loss: 0.3757\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3566 - val_loss: 0.3743\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4048 - val_loss: 0.3707\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3678 - val_loss: 0.3716\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3742 - val_loss: 0.3696\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3647 - val_loss: 0.3662\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3575 - val_loss: 0.3695\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3716 - val_loss: 0.3899\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3709 - val_loss: 0.3685\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.3688 - val_loss: 0.3684\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.3600 - val_loss: 0.3614\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3594 - val_loss: 0.3592\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3570 - val_loss: 0.3549\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3564 - val_loss: 0.3574\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3495 - val_loss: 0.3635\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3591 - val_loss: 0.3543\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3573 - val_loss: 0.3616\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3493 - val_loss: 0.3550\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3593 - val_loss: 0.3651\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3453 - val_loss: 0.3614\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3694 - val_loss: 0.3552\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.3526 - val_loss: 0.3502\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3598 - val_loss: 0.3535\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3455 - val_loss: 0.3541\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3656 - val_loss: 0.3538\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3479 - val_loss: 0.3538\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.3566 - val_loss: 0.3584\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3770 - val_loss: 0.3510\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3540 - val_loss: 0.3509\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3600\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3542 - val_loss: 0.3539\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3472 - val_loss: 0.3613\n",
      "121/121 [==============================] - 0s 476us/step - loss: 0.3557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [-4.73981231e-01 -2.53647008e+14 -3.81244421e-01 -1.56480448e+09\n",
      " -3.42327575e-01 -3.33185891e-01             nan -4.66964444e-01\n",
      " -4.18111001e+01             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7faafb55ca00>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-93724114f6ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n\u001b[0m\u001b[1;32m      2\u001b[0m                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
      "\u001b[0;32m/opt/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0;31m# we clone again after setting params in case some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# of the params are estimators as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[1;32m    877\u001b[0m                 **self.best_params_))\n\u001b[1;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/MachineLearning/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[1;32m     86\u001b[0m                                \u001b[0;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                                (estimator, name))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7faafb55ca00>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access the Best Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0012317163013458553, 'n_hidden': 3, 'n_neurons': 82}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.33318589131037396"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zooming: Initially taking very wide range of parameters, then taking parameters close to the best wide range parameter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources for Hyperparameter Tuning**\n",
    "\n",
    "Libraries\n",
    "* Hyperopt\n",
    "* Hyperas, Kopt, Talos\n",
    "* Keras Tuner\n",
    "* Scikit-Optimize\n",
    "* Spearmint\n",
    "* Hyperband\n",
    "* Sklearn-Deep\n",
    "\n",
    "Cloud Platforms\n",
    "* Google Cloub AI Tuning\n",
    "* Arimo\n",
    "* SigOpt\n",
    "* CallDesk's Oscar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers\n",
    "\n",
    "* More Number of Layers are better than single layer with large number of neurons.\n",
    "* This is because real world data is hierarchical, and deep networks are better able to exploit this than wide networks.\n",
    "* **Strech Pants Approach:** It is a good practice to try to increase the layers untill the model starts to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neurons\n",
    "\n",
    "* The Number of neurons in input layer is determined by the number shape of data.\n",
    "* The Number of neurons in output layer is determined by the type of task.\n",
    "* It is common to design deep layers in pyramid structure, though this practice hasn't been found to produce significantly better results than having same number of neurons in all the deep layers.\n",
    "* **Strech Pants Approach:** It is a good practice to increase the number of neurons until the model starts to overfit (use early stopping/ other regularization techniques to detect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: In general increasing number of layers will give better results than increasing number of neurons per layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "\n",
    "* Most Important Hyperparamter.\n",
    "* Optimal Learning Rate = Half of Maximum Learning Rate (Learning Rate at which Model Diverges)\n",
    "* Method to select Best Learning Rate:\n",
    "    * Train the Model with variable Learning Rates starting from small values (10^-5) to large values (10). This can be done by multiplying lr in each iteration by a constant factor (like `exp(log(10^6)/500)`) to go from 10^-5 to 10 in 500 Iterations.\n",
    "    * Plot the results loss as a function of Learning Rate (in log scale)\n",
    "    * Intially the loss will reduce, but after a while it will start to incease again (this is called as turning point).\n",
    "    * The Best Learning Rate is just below the turning point (10 times lower than the turning point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "*More details in Chapter 11.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Size\n",
    "\n",
    "* Larger Batches might seem better as they are able to utilize GPUs etc properly.\n",
    "* But, Larger Batches are prone to instabilities in the beginning. (Generalization Gap)\n",
    "* Research indicates that batch sizes between 2 and 32 are ideal as they take less time to train and are genrally generalized.\n",
    "* Other reseaches have also indicated that large batch sizes can give good results, when accompanied by other techniques like warming up learning rate (gradually incrasing it). These result in very less training time and no significant Generalization Gap.\n",
    "* Thus, good approach is to take large batch size in the starting, if the training is unstable, then decrease the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "\n",
    "* Step Function (Linear hence Gradient Descent cannot work)\n",
    "* Sigmoid (sigma(z) = 1/(1+exp(-z))\n",
    "* Hyperbolic Tangent (tanh(z) = 2sigma(2z) -1)\n",
    "* ReLU (Rectified Linear Unit Function) (ReLU(z) = max(0, z))\n",
    "* Soft Plus (softplus(z) = log(1_exp(z)))\n",
    "\n",
    "\n",
    "*ReLU is a good default for Dense Layers. The output Activation Function will depend on the task.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Iterations\n",
    "* Best approach is to choose large iterations with *Early Stopping*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Changing One Hyperparamter will also require modifying other hyperparameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Hopfield Network\n",
    "\n",
    "A Hopfield network which operates in a discrete line fashion or in other words, it can be said the input and output patterns are discrete vector, which can be either binary 0,1 or bipolar +1,−1 in nature. The network has symmetrical weights with no self-connections i.e., wij = wji and wii = 0.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "- This model consists of neurons with one inverting and one non-inverting output.\n",
    "\n",
    "- The output of each neuron should be the input of other neurons but not the input of self.\n",
    "\n",
    "- Weight/connection strength is represented by wij.\n",
    "\n",
    "- Connections can be excitatory as well as inhibitory. It would be excitatory, if the output of the neuron is same as the input, otherwise inhibitory.\n",
    "\n",
    "- Weights should be symmetrical, i.e. wij = wji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basis Function Network (RBFN)\n",
    "\n",
    "Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed. An RBF network is a type of feed forward neural network composed of three layers, namely the input layer, the hidden layer and the output layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
